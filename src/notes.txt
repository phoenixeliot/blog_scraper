architecture for generalized scraper

some things that vary from site to site:
- how to locate the post content (what selectors to use)
- maybe there's some stuff to exclude ("tweet this" buttons, etc)
- is there an index, or just next/previous buttons?
- domain name
- url scheme

ideal version would have visual UI (like with a chrome extension) for selecting what to include/exclude,
and how to find index links, rather than typing out selectors (not very user-friendly)

save separate configs for each blog that can be run separately

preview html output before converting to epub

replacing interlinks — needs much better link awareness (eg relative, absolute, checking domain, has a hash). In a chrome extension I could use native URL objects, I think. Maybe python has something like this?

## Link stuff
- scan document for all links
  - immediately convert them into fully verbose links (domain and absolute path, not relative, not just a path)
- scan document for all ids
  - internal links might point to these ids
  - need to stay paired with the path of the page they're on
- if a link points to an internal hash/id, the result will need that replaced with a unique hash only (eg /chap4#subsection -> "chap4_subsection")
- if a link points to an internal page, with no has, the result will need to be a unique hash to that page (eg "chap4")
- "scan the document for links" should happen on each page scrape, or at least keep the pages stored separately before concatenating them all together, with URL information and stuff



## clearly separate tasks (to maybe put in separate files/classes/whatever)
- Read the config for the blog we're scraping
- manage modes [TOC] vs [Incremental (clicking 'next')]
- TOC mode
  - scrape the TOC
  - parse the TOC into an object to iterate over
  - scrape the pages from the TOC
- Incremental mode
  - scrape the first page
  - get the link to the next page (in addition to other processing?)
  - scrape the next page, continue until done
- Process a page for links, including internal hashed links
  - Keep a central set of links, and the pages they're from
- Process a page for referenced images [optional, eg on first runs. Maybe cache the images between runs..?]
  - scrape the images, save them somewhere central
- Post-processing
  - Isolate the content we care about (include/exclude certain selectors)
  - Give internal links unique hash IDs
  - Replace IDs in the document to match
  - Replace hrefs in the document to match
  - Replace converted images with their base64 data
- Write out the content to a file


## Architecture/flow:
Entry: main
- reads config, CLI arguments
- picks appropriate mode to run
- runs that mode, takes its output
- feeds output into file-writing code

manager for TOC mode
- use the config's TOC location to scrape that page
- process TOC page for links using link-extractor
- queue up individual page scrape jobs based on TOC links
- for each page scrape, do full link extraction
  - also queue up image scrapes/extractions
- once all pages are scraped and processed, feed content into post-processing

manager for Incremental mode
- use the config's first page location to scrape that page
- process that page for the next page link using link-extractor
  - also, separately, do full link extraction from this page
  - (convert this page into an easy to access object _once_)
- scrape the next page, process it like the last
- once there are no more next pages, and no more processing jobs to do, feed content into post-processing

link extractor
-? where do I convert the scraped page into a navigable DOM-like object? If in JS, it's just like that already.
- find all the <a> tags in the page, extract their hrefs
- extracted link object:
  - properties:
    - page it was extracted from
    - full path it points to, including hash
  -? where do I mark whether it seems to be pointing internally? in the extractor, or elsewhere?
  - kinds of links
    - hash links within the same chapter/page
    - hash links to other pages on the blog
    - non-hash links to other pages on the blog
    - links to the outside
    - javascript links (ignore these?? eg toggle archive)

page scraper
- given a URL, downloads the content.
- returns a parsed DOM thing? TODO: figure out appropriate python libraries for this
- two modes: urllib, chromedriver
  - with chromedriver, don't necessarily know when the page is done.
    maybe have a thing we're looking for, and wait for that to exist/have content?

image scraper
- given a URL, downloads the content.
- returns a base64 encoded image?



## Sketching more centralized link replacement
- TOC links get swapped with #chap1 etc
- in-page links get swapped with #chap3_someid
- links from other pages get swapped with #chap3_someid

TOC and all pages need to extract all links (paired with source_urls), and feed them into a central link LUT
- map from [full URL with hash] to [hash_id for new link]
- hash_id depends on previous hash and chapter the linked section is part of (chapter could be inferrable URl's path)

Then we need to replace every link's href, and update id's for preexisting id'd tags, and add ids to chapter starts
- store every link in a list, then iterate over it and edit the tag
-- the multicore scraper thing(?) doesn't like putting tags on my Link objects
-- do I even need my custom link objects?? probably not
- for each TOC link
-- update the href to point to the chapter hash_id
- for each content link
-- update the href to point to the chapter hash_id, or subsection hash_id

Every [chapter, linked id'd tag] will need a hash_id
- chapters get #chapN
- linked id'd tags get #chapN_thingy
-- this requires keeping context for which chapter they're part of. we could just not.
- need to make all of these unique
-- could just keep a global incrementing ID but that's not very Clean




## Things not yet handled
- hosting-specific modes (eg blogspot has weird archive-by-month/year stuff)
- TOC has links to eg "all posts in october" to ignore (I think blogspot does this? or maybe just agenty duck)
- Embed the original TOC text with the links updated (so you get stuff like Ward's description of the prologue)
- Embed footnote-like things for external links, so you can see that it's external and what the URL is before opening it
- Some links will redirect — we want to back-substitute the final URL into all links that go there
  - eg https://www.parahumans.net/2017/10/26/glow-worm-0-03/ -> https://www.parahumans.net/2017/10/26/glow-worm-0-3/
- Clean up empty tags (eg <strong>&nbsp; &nbsp; &nbsp; </strong>, which is left over after removing "Next Chapter"s in Ward)
- Could support python config files, which would allow providing a cleanup function inside the config, for site-specific cleanup




dependencies:
uritools
pyyaml
bs4
lxml
