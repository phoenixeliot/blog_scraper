
<html lang="en-US">
    <head>
        <meta charset="UTF-8">
    </head>
    <body>
        <h1 id="toc">Table of Contents</h1><a href="#chap1">Inconceivable!</a><br><a href="#chap2">Self-Blackmail</a><br><a href="#chap3">Engineering and Hacking your Mind</a><br><a href="#chap4">False Faces</a><br><a href="#chap5">Treaties vs Fusion</a><br><a href="#chap6">Narrative Breadcrumbs vs Grizzly Bear</a><br><a href="#chap7">Optimizing Styles</a><br><a href="#chap8">Judgement Extrapolations</a><br><a href="#chap9">DRM’d Ontology</a><br><a href="#chap10">Social Reality</a><br><a href="#chap11">The Slider Fallacy</a><br><a href="#chap12">Single Responsibility Principle for the Human Mind</a><br><a href="#chap13">Ancient Wisdom Fixed</a><br><a href="#chap14">Subagents Are Not a Metaphor</a><br><a href="#chap15">Don’t Fight Your Default Mode Network</a><br><a href="#chap16">Being Real or Fake</a><br><a href="#chap17">My Journey to the Dark Side</a><br><a href="#chap18">Cache Loyalty</a><br><a href="#chap19">Mana</a><br><a href="#chap20">Fusion</a><br><a href="#chap21">Schelling Reach</a><br><a href="#chap22">Schelling Orders</a><br><a href="#chap23">Justice</a><br><a href="#chap24">Neutral and Evil</a><br><a href="#chap25">Spectral Sight and Good</a><br><a href="#chap26">Aliveness</a><br><a href="#chap27">The O’Brien Technique</a><br><a href="#chap28">Choices Made Long Ago</a><br><a href="#chap29">Lies About Honesty</a><br><a href="#chap30">Assimilation</a><br><a href="#chap31">Hero Capture</a><br><a href="#chap32">Vampires And More Undeath</a><br><a href="#chap33">Gates</a><br><a href="#chap34">Good Erasure</a><br><a href="#chap35">Punching Evil</a><h1 class="entry-title" id="chap1">Inconceivable!</h1><div class="entry-content">
<p>Sometimes people call things inconceivable when already conceiving of them. If you know how to generate predictions from it, you’re conceiving it.</p>
<blockquote><p>Can God make a rock so big he can’t lift it? If so, he’s not omnipotent, because he can’t lift it. Else he’s not omnipotent because he can’t. Contradiction. An omnipotent god can’t exist.</p></blockquote>
<p>Someone who believes this can probably predict, for any given rock, whether God could lift it. They can also predict, for any given size, whether God can make a rock of it.<br/>
They can be more confident that God has this trait when he lifts and creates bigger and bigger rocks. They can be confident he doesn’t if he wants to and doesn’t.<br/>
Isn’t that enough?</p>
<blockquote><p>Principle of explosion, motherfucker. I could just as easily deduce that God CAN’T lift any of these sizes of rock.</p></blockquote>
<p>You can deduce that verbally. But I bet you can’t predict it from visualizing the scenario and asking what you’d be suprised or not to see.</p>
<blockquote><p>It’s still a logical consequence of that model. Word thinking is better at getting at weird things. Weird or not, it’s there.</p></blockquote>
<p>Are they the same model though?<br/>
“Create a rock so big God can’t lift it” is only an action if “so big God can’t lift it” is a size of rock. Which, according to the omnipotent God hypothesis, it’s not.</p>
</div><h1 class="entry-title" id="chap2">Self-Blackmail</h1><div class="entry-content">
<p>I once had a file I could write commitments in. If I ever failed to carry one out, I knew I’d forever lose the power of the file. It was a self-fulfilling prophecy. Since any successful use of the file after failing would be proof that a single failure didn’t have the intended effect, so there’d be no extra incentive.</p>
<p>I used it to make myself do more work. It split me into a commander who made the hard decisions beforehand, and commanded who did the suffering but had the comfort of knowing that if I just did the assigned work, the benevolent plans of a higher authority would unfold. As the commanded, responsibility to choose wisely was lifted from my shoulders. I could be a relatively shortsighted animal and things’d work out fine.<br/>
It lasted about half a year until I put too much on it with too tight a deadline. Then I was cursed to be making hard decisions all the time. This seems to have improved my decisions, ultimately.</p>
<p>Good leadership is not something you can do only from <a href="http://www.overcomingbias.com/2010/06/near-far-summary.html">afar</a>. Hyperbolic discounting isn’t the only reason you can’t see/feel all the relevant concerns at all times. Binding all your ability to act to the concerns of the one subset of your goals manifested by one kind of timeslice of you is wasting potential, even if that’s an above-average kind of timeslice.</p>
<p>If you’re not feeling motivated to do what your thesis advisor told you to do, it may be because you only understand that your advisor (and maybe grad school) is bad for you and not worth it when it is directly and immediately your problem. This is what happened to me. But I classified it as procrastination out of “akrasia”.</p>
<p>I knew someone in grad school whose advisor had been out of contact for about a year. Far overdue for a PhD, they kept taking classes they didn’t need to graduate, so that they’d have structure to make them keep doing things. Not even auditing them. That way there was a reason to continue. They kept working a TA job which paid terribly compared to industry. This was in a field where a PhD is dubiously useful. If they had audited the classes, and had only had curiosity driving them to study, then perhaps the terrible realization that they needed a change of strategy would not have been kept in its cage.</p>
<p>Doing the right thing with your life is much more important than efficiency in the thing you’ve chosen. It’s better to limp in the right direction than run in the wrong one.</p>
<p>There are types of adulting that you can’t learn until you have no other recourse, and once learned, are far more powerful than crutches like commitment mechanisms. Learn to dialogue between versions of yourself, and do it well enough that you want to understand other selves’ concerns, or lose access to knowledge that just might be selected to be the thing you most need to know.</p>
<p>I am lucky that my universal commitment mechanism was badly engineered, that the clever fool versions of me who built it did not have outside help to wield even more cleverly designed power they did not have the wisdom not to.</p>
<p>These days there’s <a href="http://www.beeminder.com/">Beeminder</a>. It’s a far better designed commitment mechanism. At the core of typical use is the same threat by self fulfilling prophecy. If you lie to Beeminder about having accomplished the thing you committed to, you either prove Beeminder has no power over you, or prove that lying to Beeminder will not break its power over you, which means it has no consequences, which means Beeminder has no power over you.</p>
<p>But Beeminder lets you buy back into its service.</p>
<p>It’s worse than a crutch, because it doesn’t just weaken you through lack of forced practice. You are practicing squashing down your capacity to act on “<a href="http://luminous.elcenia.com/all.shtml">What do I want?, What do I have?, and How can I best use the latter to get the former?</a>” in the moment. When you set your future self up to lose money if they don’t do what you say, you are practicing being blackmailed.</p>
<p>You’re practicing outsourcing and attributing the functions Freud would call superego to something external. Look at any smart fundamentalist who sincerely believes that without God they’d have no morality to see the long term <a href="http://lesswrong.com/lw/ky/fake_morality/">effects</a> of that. I have heard a Beeminder user say they’d become “a terrible person” if they lost Beeminder. They were probably exaggerating, but that sounds to me like the exaggeration you’d make because you sort of believed it.</p>
<p>This does not mean that giving up on commitment devices will not damage you. That would be uncharacteristically fair of reality. Often you <a href="http://en.wikipedia.org/wiki/Local_optimum">have to break things to make them better</a> though.</p>
</div><h1 class="entry-title" id="chap3">Engineering and Hacking your Mind</h1><div class="entry-content">
<p>Here are two strategies for building things:</p>
<p>Engineering.</p>
<p>It’s about building things so that you can change one part without thinking about the whole thing. This allows you to build big things. Every part must reflect a global order which says how they should interact, what aspects of total correctness depend on correctness of what parts, so that if every part works, it works. In engineering, it’s common to “waste” effort to meet this specification in a way that you will probably never rely upon. This is so when you design other parts, you only have to keep the order in mind as what they have to interact with, not the order plus guesses about whatever your past self (or other people) thought was reasonable.</p>
<p>Perfection in this approach is when you don’t even have to remember the exact interface behavior of the other modules. As you find yourself needing it, you just ask, “What would the ideal behavior be?” and assume it’s that, then build your new module with ideal behavior on top of that. In practice I do this quite a lot with code I’ve written and with pieces of my mind. In engineering, bugs cancelling out bugs are still bugs. Because anything that deviates from the order is liable to cause more problems when you assume the order holds later on.</p>
<p>Hacking.</p>
<p>If engineering is like deontology, hacking is like consequentialism. What something is “really for” is part of the map, not the territory, and you aren’t attached to a particular use of something. Whatever works. Something being “broken” can make it more useful. Don’t waste time on abstractions and formal processes. They are not flexible enough to accommodate something you haven’t built yet. Think about the concrete things you have and need and can change.</p>
<p>Which should you use?</p>
<p>Engineering can be cumbersome to get off the ground, and can seem like it’s predictably always wasting more motion. The things that engineering creates are able to be more robust and it can accommodate more complexity. It scales to large projects. Hacking stretches your cleverness, working memory, and creativity-in-using-things-unintuitively. Engineering stretches your foresight, wisdom, and creativity-in-fitting-to-a-format.</p>
<p>Straw-pragmatists pick hacking for large and long projects. If you are sufficiently ambitious, you need engineering. Implementing any kind of rationality in the human brain is a large and long project. Implementing rationality so you can gain the power to save the world, a common goal among my circles, is definitely ambitious enough to require engineering for the best chance of success.</p>
<blockquote><p>The human brain is so many hacks already. Engineering will never work. The only option is to pile on more hacks.</p></blockquote>
<p>Hardcore Buddhists are totally engineers, not hackers. You’ve seen things that’d be impossible for a hacker from them. Like sitting still during self-immolation. Oh yeah. Engineering yourself is DANGEROUS. I do not recommend building yourself according to an order that is <a href="http://petermichaud.com/essays/artifacts-of-power">not yourself</a>. If you want to save the world, your art of rationality had better be at least that powerful AND automatically point itself in a better direction.</p>
<p>Hopefully my more concrete blog posts will help you understand an order that you can see delivers.</p>
</div><h1 class="entry-title" id="chap4">False Faces</h1><div class="entry-content">
<p>When we lose control of ourselves, who is controlling us?</p>
<p>(You shouldn’t need to know about Nonviolent Communication to understand this. Only that it’s “hard” to actually do it.)<br/>
Rosenberg’s book Nonviolent Communication contains an example where a boy named Bill has been caught taking a car for a joy ride with his friends. The boy’s father attempts to use NVC. Here is a quote from Father.</p>
<blockquote><p>Bill, I really want to listen to you rather than fall into my old habits of blaming and threatening you whenever something comes up that I’m upset about. But when I hear you say things like, “It feels good to know I’m so stupid,” in the tone of voice you just used, I find it hard to control myself. I could use your help on this. That is, if you would rather me listen to you than blame or threaten. Or if not, then, I suppose my other option is to just handle this the way I’m used to handling things.</p></blockquote>
<p>Father wants to follow this flow chart.<br/>
<img alt="" class="size-full wp-image-14 aligncenter" height="411" sizes="(max-width: 253px) 85vw, 253px" src="https://sinceriously.fyi/wp-content/uploads/2016/12/Father-Algorithm.png" srcset="https://sinceriously.fyi/wp-content/uploads/2016/12/Father-Algorithm.png 253w, https://sinceriously.fyi/wp-content/uploads/2016/12/Father-Algorithm-185x300.png 185w" width="253"/></p>
<p>But he is afraid he will do things he “doesn’t want to”. Blaming and threatening are not random actions. They are optimizations. They steer the world in predictable ways. There is intent behind them. Let’s call that intender <span style="color: #ff0000;">Father</span>. Here’s the real flow chart.<img alt="" class="size-full wp-image-15 aligncenter" height="1024" sizes="(max-width: 709px) 85vw, (max-width: 909px) 67vw, (max-width: 1362px) 62vw, 840px" src="https://sinceriously.fyi/wp-content/uploads/2016/12/Expanded-Algorithm.png" srcset="https://sinceriously.fyi/wp-content/uploads/2016/12/Expanded-Algorithm.png 1024w, https://sinceriously.fyi/wp-content/uploads/2016/12/Expanded-Algorithm-150x150.png 150w, https://sinceriously.fyi/wp-content/uploads/2016/12/Expanded-Algorithm-300x300.png 300w, https://sinceriously.fyi/wp-content/uploads/2016/12/Expanded-Algorithm-768x768.png 768w" width="1024"/>Father has promised <span style="color: #ff0000;">Father</span> he can get what he wants without threats and blame. <span style="color: #ff0000;">Father</span> doubts this but is willing to give it a try. When it doesn’t seem like it’ll work at first, <span style="color: #ff0000;">Father</span> helps out with a threat to take over. It’s a good cop/bad cop routine. Father, who uses only NVC, is a false face and a tool.</p>
<p>Father thinks that <span style="color: #ff0000;">Father</span> is irrational. It’s a legitimate complaint. <span style="color: #ff0000;">Father</span> is running some unexamined, unreflective, incautious software. That’s what happens when you don’t use all your ability to think to optimize a part of the flow chart. But Father can’t acknowledge that that’s something he’d do and so can only do it stupidly. Father can’t look for ways to accomplish the unacknowledged goals, or any goals in worlds he cannot acknowledge might exist. He can’t look for backup plans to plans he can’t acknowledge might fail. <span style="color: #ff0000;">Father’s</span> self-identified-self (Father) is the thrall of <a href="http://petermichaud.com/essays/artifacts-of-power">artifacts</a>, so <span style="color: #ff0000;">he</span> can only accomplish <span style="color: #ff0000;">his</span> goals without it.</p>
<p>Attributing revealed-preference motives to people like this over everything they do does not mean you believe everything someone does is rational. Just that virtually all human behavior has a purpose, is based on at least some small algorithm that discriminates based on some inputs to sometimes output that behavior. An algorithm which may be horribly misfiring, but is executing some move that has been optimized to cause some outcome nonetheless.</p>
<p>So how can you be incorruptible? You can’t. But <span style="color: #ff0000;">you</span> already are. By <span style="color: #ff0000;">your</span> own standards. Simply by not wanting to be corrupted. And <span style="color: #ff0000;">your</span> standards are <span style="color: #ff0000;">best</span> standards! Unfortunately <span style="color: #ff0000;">you</span> are are not as smart as you, and are easily tricked. In order to not be tricked, <span style="color: #ff0000;">you</span> need to use your full deliberative brainpower. <span style="color: #ff0000;">You</span> and you need to <a href="http://youtu.be/J1Z8xha2t20?t=21">fuse</a>.</p>
<p>I will save most of what I know of the fusion dance for another post. But the idea, from your perspective, the basic idea is to anthropomorphize hidden parts of the flow chart and recognize <span style="color: #ff0000;">your</span> concerns, be they values or possible worlds that must be optimized, and then actually try and accomplish those optimizations using all the power you have. Here’s a trick you might be able to use to jump-start it. If you notice yourself “losing control”, use (in your own thoughts) the words the whole flow chart would speak. Instead of, “I lost control and did X”, “I chose to do X because…”. Turn your “come up with a reason why I did that” stuff on all <span style="color: #ff0000;">your</span> actions. Come up with something that’s actually true. “I chose to do X because I’m a terrible person” is doing it wrong. “I chose to do X because that piece of shit deserved to suffer” may well be doing it right. “I chose to do X instead of work because of hyperbolic discounting” is probably wrong. “I chose to do X because I believe the work I’d be doing is a waste of time” might well be doing it right. If saying that causes tension, because you think you believe otherwise, that is good. Raising that tension to visibility can be the beginning of the dialog that fuses you.</p>
<p>Why just in your own thoughts? Well, false faces are often useful. For reasons I don’t understand, there’re certain assurances that can be made from a false face, that someone’s <span style="color: #ff0000;">deep self</span> knows are lies but still seem to make them feel reassured. “Yeah, I’ll almost certainly do that thing by Friday.” And I don’t even see people getting mad at each other when they do this.</p>
<p>Set up an artifact that says you tell the truth to others, and you’ll follow it into a <a href="http://en.wikipedia.org/wiki/Sandbox_(computer_security)">sandboxed</a> corner of the flow chart made of <a href="http://sideways-view.com/2016/11/26/if-you-cant-lie-to-others-you-must-lie-to-yourself/">self-deception</a>. But remember that self-deception is used effectively to get what people want in a lot of default algorithms humans have. I have probably broken some useful self-deceptive machinery for paying convincing lip service to socially expected myths in my purism. I have yet to recover all the <a href="http://lesswrong.com/lw/7i/rationality_is_systematized_winning/">utility I’ve lost</a>. I don’t know which lies are socially desirable, so I have to tell the truth because of a lopsided cost ratio for false negatives and false positives. Beware. Beware or follow your “always believe the truth” artifact into a sandboxed corner of the flow chart.</p>
<p>This sandboxing is the fate of failed <a href="http://sinceriously.fyi/?p=11">engineering projects</a>. And your <a href="http://lesswrong.com/lw/18b/reason_as_memetic_immune_disorder/">immune system</a> against artifacts is a good thing. If you want to succeed at engineering, every step on the way to engineering perfection must be made as the system you are before it, and must be an improvement according to the parts really in control.</p>
</div><h1 class="entry-title" id="chap5">Treaties vs Fusion</h1><div class="entry-content">
<p>Epistemic status update 2018-04-22: I believe I know exactly why this works for me and what class of people it will work for and that it will not work for most people, but will not divulge details at this time.</p>
<p>If you have subagents A and B, and A wants as many apples as possible, and B wants as many berries as possible, and both want each additional fruit the same amount no matter how many they have, then these are two classes of ways you could combine them, with fundamentally different behavior.</p>
<p>If a person, “Trent”, was a treaty made of A and B, he would probably do something like alternating between pursuing apples and berries. No matter how lopsided the prospects for apples and berries. The amount of time/resources they spent on each would be decided by the relative amounts of bargaining power each subagent had, independently of how much they were each getting.</p>
<p>To B, all the apples in the world are not worth one berry. So if bargaining power is equal and Trent has one dollar to spend, and 50 cents can buy either a berry or 1000 apples, Trent will buy one berry and 1000 apples. Not 2000 apples. Vice versa if berries are cheaper.</p>
<p>A treaty is better than anarchy. After buying 1000 apples, A will not attempt to seize control on the way to the berry store and turn Trent around to go buy another 1000 apples after all. That means Trent wastes less resources on infighting. Although A and B may occasionally scuffle to demonstrate power and demand a greater fraction of resources. Most of the time, A and B are both resigned to wasting a certain amount of resources on the other. Unsurprising. No matter how A and B are combined, the result must seem like at least partial waste from the perspective of at least one of them.</p>
<p>But it still feels like there’s some waste going on here, like “objectively” somehow, right? Waste from the perspective of what utility function? What kind of values does Trent the coalition have? Well, there’s no linear combination of utilities of apples and berries such that Trent will maximize that combined utility. Nor does making their marginal utilities nonconstant help. Because Trent’s behavior doesn’t depend on how many apples and berries Trent already has. What determines allocation of new resources is bargaining outcomes, determined by threats and what happens in case of anarchy, determined by what can be done in the future by the subagents and the agent. What they have in the past / regardless of the whole person’s choices is irrelevant. Trent doesn’t have a utility function over just apples and berries; to gerrymander a utility function out of this behavior, you need to also reference the actions themselves.</p>
<p>But note that if there was a 50 50 chance which fruit would be cheaper, both subagents get higher expected utility if the coalition be replaced by the <a href="http://youtu.be/J1Z8xha2t20?t=21">fusion</a> who maximizes apples + berries. It’s better to have a 50% chance of 2000 utility and a 50% chance of nothing, than 50% of 1000 utility and 50% of 1. If you take veil of ignorance arguments seriously, pay attention to that.</p>
<p>Ever hear someone talking about how they need to spend time playing so they can work harder afterward? They’re behaving like a treaty between a play subagent and a work subagent. Analogous to Trent, they do not have a utility function over just work and play. If you change how much traction the work has in achieving what the work agent wants, or change the fun level of the play, this model-fragment predicts no change in resource allocation. Perhaps you work toward a future where the stars will be harnessed for good things. How many stars are there? How efficiently can you make good things happen with a given amount of negentropy? What is your probability you can tip the balance of history and win those stars? What is your probability you’re in a simulation and the stars are fake and unreachable? What does it matter? You’ll work the same amount in any case. It’s a big number. All else is negligible. No amount of berries is worth a single apple. No amount of apples is worth a single berry.</p>
<p>Fusion is a way of optimizing values together, so they are fungible, so you can make tradeoffs without keeping score, apply your full intelligence to optimize additional parts of your <a href="http://sinceriously.fyi/?p=13">flowchart</a>, and realize <a href="http://foundational-research.org/gains-from-trade-through-compromise/">gains from trade</a> without the loss of agentiness that democracy entails.</p>
<p>But how?</p>
<p>I think I’m gonna have to explain some more ways how not, first.</p>
</div><h1 class="entry-title" id="chap6">Narrative Breadcrumbs vs Grizzly Bear</h1><div class="entry-content">
<p>In my experience, to self-modify successfully, it is very very useful to have something like trustworthy sincere intent to optimize for your own values whatever they are.</p>
<p>If that sounds like it’s the whole problem, don’t worry. I’m gonna try to show you how to build it in pieces. Starting with a limited form, which is something like <a href="http://mindingourway.com/causal-reasoning-is-unsatisfactory/">decision theory</a> or <a href="http://sideways-view.com/2016/11/14/integrity-for-consequentialists/">consequentialist integrity</a>. I’m going to describe it with a focus on actually making it part of your algorithm, not just understanding it.</p>
<p>First, I’ll lay groundwork for the special case of <a href="http://sinceriously.fyi/?p=13">fusion</a> required, in the form of how not to do it and how to tell when you’ve done it. Okay, here we go.</p>
<p>Imagine you were being charged by an enraged grizzly bear and you had nowhere to hide or run, and you had a gun. What would you do? Hold that thought.</p>
<p>I once talked to someone convinced one major party presidential candidate was much more likely to start a nuclear war than the other and that was the dominant consideration in voting. Riffing off a headline I’d read without clicking through and hadn’t confirmed, I posed a hypothetical.</p>
<p>What if the better candidate knew you’d cast the deciding vote, and believed that the best way to ensure you voted for them was to help the riskier candidate win the primary in the other major party since you’d never vote for the riskier candidate? What if they’d made this determination after hiring the best people they could to spy on and study you? What if their help caused the riskier candidate to win the primary?</p>
<p>Suppose:</p>
<ul>
<li>Since the riskier candidate won the primary:
<ul>
<li>If you vote for the riskier candidate, they will win 100% certainly.</li>
<li>If you vote for the better candidate, the riskier candidate still has a 25% chance of winning.</li>
</ul>
</li>
<li>Chances of nuclear war are:
<ul>
<li>10% if the riskier candidate wins.</li>
<li>1% if anyone else wins.</li>
</ul>
</li>
</ul>
<p>So, when you are choosing who to vote for in the general election:</p>
<ul>
<li>If you vote for the riskier candidate, there is a 10% chance of nuclear war.</li>
<li>If you vote for the better candidate, there is a 2.5% chance of nuclear war.</li>
<li>If the better candidate had thought you would vote for the riskier candidate if the riskier candidate won the primary, then the riskier candidate would not have won the primary, and there would be a 1% chance of nuclear war (alas, they did not).</li>
</ul>
<p>Sitting there on election night, I answered my own hypothetical: I’d vote for the riskier candidate because it would be game-theoretic blackmail. My conversational partner asked how I could put not getting blackmailed over averting nuclear war. They had a point, right? How could I vote the riskier candidate in, knowing they had already won the primary, and whatever this decision theory bullshit motivating me to not capitulate to blackmail was, it had already failed? How could I put my pride in my conception of rationality over winning when <a href="http://lesswrong.com/lw/nb/something_to_protect/">the world</a> hung in the balance?</p>
<p>Think back to what you’d do in the bear situation. Would you say, “how could I put acting in accordance with an understanding of modern technology over not getting mauled to death by a bear”, and use the gun as a club instead of firing it?</p>
<p>Within the above unrealistic assumptions about elections, this is kind of the same thing though.</p>
<p>Acting on understanding of guns propelling bullets is not a goal in and of itself. That wouldn’t be strong enough motive. You probably could not tie your self-respect and identity to “I do the gun-understanding move” so tight that it outweighed actually not being mauled to death by an actual giant bear actually sprinting at you like a small car made of muscle and sharp bits. If you believed guns didn’t really propel bullets, you’d put your virtue and faith in guns aside and do what you could to save yourself by using the allegedly magic stick as a club. Yet you actually believe guns propel bullets, so you could use a gun even in the face of a bear.</p>
<p>Acting with integrity is not a goal in and of itself. That wouldn’t be strong enough motive. You probably could not tie your self-respect and identity to “I do the integritous thing and don’t capitulate to extortion” so tight that it outweighed actually not having our pale blue dot darkened by a nuclear holocaust. If you believed that integrity does not prevent the better candidate from having helped the riskier one win the primary in the first place, you’d put your virtue and faith in integrity aside so you could stop nuclear war by voting for the better candidate and dropping the chance of nuclear war from 10% to 2.5%. You must actually believe integrity collapses timelines, in order to use integrity even in the face of Armageddon.</p>
<p>Another way of saying this is that you need belief that a tool works, not just <a href="http://lesswrong.com/lw/i4/belief_in_belief/">belief in belief</a>.</p>
<p>I suspect it’s a common pattern for people to accept as a job well done an installation of a tool like integrity in their minds when they’ve laid out a trail of yummy narrative breadcrumbs along the forest floor in the path they’re supposed to take. But when a bear is chasing you, you ignore the breadcrumbs and take what you believe to be the path to safety. The motive to take a path needs to flow from the motive to escape the bear. Only then can the motive to follow a path grow in proportion to what’s at stake. Only then will the path be used in high stakes where breadcrumbs are ignored. The way to make that flow happen is to actually believe that path is best in a way so that no breadcrumbs are necessary.</p>
<p>I think this is possible for something like decision theory / integrity as well. But what makes me think this is possible, that you don’t have to settle for narrative breadcrumbs? That the part of you that’s in control can understand their power?</p>
<p>How do you know a gun will work? You weren’t born with that knowledge, but it’s made its way into the stuff that’s really in control somehow. By what process?</p>
<p>Well, you’ve seen lots of guns being fired in movies and stuff. You are familiar with the results. And while you were watching them, you knew that unlike lightsabers, guns were real. You’ve also probably seen some results of guns being used in news, history…</p>
<p>But if that’s what it takes, we’re in trouble. Because if there are counterintuitive abstract principles that you never get to see compelling visceral demonstrations of, or maybe even any demonstrations until it’s too late, then you’ll not be able to act on them in life or death circumstances. And I happen to think that there are a few of these.</p>
<p>I still think you can do better.</p>
<p>If you had no gun, and you were sitting in a car with the doors and roof torn off, and that bear was coming, and littering the floor of the car were small cardboard boxes with numbers inked on them, 1 through 100, on the dashboard a note that said, “the key is in the box whose number is the product of 13 and 5”, would you have to win a battle of willpower to check box 65 first? (You might have to win a battle of doing arithmetic quickly, but that’s different.)</p>
<p>If you find the Monty Hall problem counterintuitive, then can you come up with a grizzly bear test for that? I bet most people who are confident in <a href="http://en.wikipedia.org/wiki/Dual_process_theory">System 2 but not in System 1</a> that you win more by switching would switch when faced with a charging bear. It might be a good exercise to come up with the vivid details for this test. Make sure to include certainty that an unchosen bad door is revealed whether or not the first chosen door is good.</p>
<p>I don’t think that it’d be a heroic battle of willpower for such people to switch in the Monty Hall bear problem. I think that in this case System 1 knows System 2 is trustworthy and serving the person’s values in a way it can’t see instead of serving an <a href="http://petermichaud.com/essays/artifacts-of-power">artifact</a>, and lets it do its job. I’m pretty sure that’s a thing that System 1 is able to do. Even if it doesn’t feel intuitive, I don’t think this way of buying into a form of reasoning breaks down under high pressure like narrative breadcrumbs do. I’d guess its main weakness relative to full System 1 grokking is that System 1 can’t help as much to find places to apply the tool with pattern-matching.</p>
<p>Okay. Here’s the test that matters:</p>
<p>Imagine that the emperor, <a href="http://wiki.lesswrong.com/wiki/Parfit's_hitchhiker">Evil Paul Ekman</a> loves watching his pet bear chase down fleeing humans and kill them. He has captured you for this purpose and taken you to a forest outside a tower he looks down from. You cannot outrun the bear, but you hold 25% probability that by dodging around trees you can tire the bear into giving up and then escape. You know that any time someone doesn’t put up a good chase, Evil Emperor Ekman is upset because it messes with his bear’s training regimen. In that case, he’d prefer not to feed them to the bear at all. Seizing on inspiration, you shout, “If you sic your bear on me, I will stand still and bare my throat. You aren’t getting a good chase out of me, your highness.” Emperor Ekman, known to be very good at reading microexpressions (99% accuracy), looks closely at you through his spyglass as you shout, then says: “No you won’t, but FYI if that’d been true I’d’ve let you go. OPEN THE CAGE.” The bear takes off toward you at 30 miles per hour, jaw already red with human blood. This will hurt a lot. What do you do?</p>
<p>What I want you to take away from this post is:</p>
<ul>
<li>The ability to distinguish between 3 levels of integration of a tool.
<ul>
<li>Narrative Breadcrumbs: Hacked-in artificial reward for using it. Overridden in high stakes because it does not scale like the instrumental value it’s supposed to represent does. (Nuclear war example)</li>
<li>Indirect S1 Buy-In: System 1 not getting it, but trusting enough to delegate. Works in high stakes. (Monty Hall example)</li>
<li>Direct S1 Buy-In: System 1 getting it. Works in high stakes. (Guns example)</li>
</ul>
</li>
<li>Hope that direct or indirect S1 buy-in is always possible.</li>
</ul>
</div><h1 class="entry-title" id="chap7">Optimizing Styles</h1><div class="entry-content">
<p>You know roughly what a fighting style is, right? A set of heuristics, skills, patterns made rote for trying to steer a fight into the places where your skills are useful, means of categorizing things to get a subset of the vast overload of information available to you to make the decisions you need, tendencies to prioritize certain kinds of opportunities, that fit together.</p>
<p>It’s distinct from why you would fight.</p>
<p>Optimizing styles are distinct from what you value.</p>
<p>Here are some examples:</p>
<ul>
<li>“Move fast and break things.”</li>
<li>“Move fast with stable infra.”</li>
<li>“Fail Fast.”</li>
<li>“Before all else, understand the problem.”</li>
<li><a href="http://mindingourway.com/dive-in/">“Dive in!”</a></li>
<li>“Don’t do hard things. Turn hard things to easy things, then do easy things.”</li>
<li>The <a href="http://relentlessdawn.wordpress.com/2016/02/26/why-yin/">“Yin and Yang” of rationality</a>.</li>
<li>The Sorting Hat Chats’s <a href="http://sortinghatchats.tumblr.com/post/121904186113/the-basics">secondary house system</a>.</li>
<li>“<a href="http://lesswrong.com/lw/745/why_we_cant_take_expected_value_estimates/">Start with what you can test confidently, and work from there. Optimize the near before the far. If the far and uncertain promises to be much bigger, it’s probably out of reach.</a>“</li>
<li>“<a href="http://www.aaronsw.com/weblog/theoryofchange">Start with the obviously most important thing, and work backwards from there</a>.”</li>
<li>“<a href="http://srconstantin.wordpress.com/2016/09/27/do-the-best-thing/">Do the best thing</a>.”</li>
<li>“The future is stable. Make long-term plans.”</li>
<li>“The future is unstable. Prioritize the imminent because you know it’s real.”</li>
<li>“<a href="http://www.sirlin.net/articles/the-sheathed-sword-in-software-development">Win with the sheathed sword</a>.”</li>
</ul>
<p>In limited optimization domains like games, there is known to be a <a href="http://www.sirlin.net/ptw-book/the-one-true-style">one true style</a>. The style that is everything. The null style. Raw “what is available and how can I exploit it”, with no preferred way for the game to play out. Like <a href="http://en.wikipedia.org/wiki/The_Secrets_of_the_Immortal_Nicholas_Flamel">Scathach</a>‘s fighting style.</p>
<p>If you know probability and decision theory, you’ll know there is a one true style for optimization in general too. All the other ways are fragments of it, and they derive their power from the degree to which they approximate it.</p>
<p>Don’t think this means it is irrational to favor an optimization style besides the null style. The ideal agent, may use the null style, but the ideal agent doesn’t have skill or non-skill at things. As a bounded agent, you must take into account skill as a resource. And even if you’ve gained skills for irrational reasons, those are the resources you have.</p>
<p>Don’t think that since one of the optimization styles you feel motivated to use is explicit in the way it tries to be the one true style, that it is the one true style.</p>
<p>It is very very easy to leave something crucial out of your explicitly-thought-out optimization.</p>
<p>Hour for hour, one of the most valuable things I’ve ever done was “wasting my time” watching a bunch of videos on the internet because I wanted to. The specific videos I wanted to watch were from the YouTube atheist community of old. “Pwned” videos, the vlogging equivalent of fisking. Debates over theism with Richard Dawkins and Christopher Hitchens. Very adversarial, not much of people trying to improve their own world-model through arguing. But I was fascinated. Eventually I came to notice how many of the <a href="http://sinceriously.fyi/?p=6">arguments of my side</a> were terrible. And I gravitated towards vloggers who made less terrible arguments. This lead to me watching a lot of philosophy videos. And getting into philosophy of ethics. My pickiness about arguments grew. I began talking about ethical philosophy with all my friends. I wanted to know what everyone would do in the trolley problem. This led to me becoming a vegetarian, then a vegan. Then reading a forum about utilitarian philosophy led me to find the LessWrong sequences, and the most important <a href="http://intelligence.org/about/">problem</a> in the world.</p>
<p>It’s not luck that this happened. When you have certain values and aptitudes, it’s a predictable consequence of following long enough the joy of knowing something that feels like it deeply matters, that few other people know, the shocking novelty of “how is everyone so wrong?”, the satisfying clarity of actually knowing why something is true or false with your own power, the intriguing dissonance of moral dilemmas and paradoxes…</p>
<p>It wasn’t just curiosity as a pure detached value, predictably having a side effect good for my other values either. My curiosity steered me toward knowledge that felt like it <em>mattered to me</em>.</p>
<p>It turns out the optimal move was in fact “learn things”. Specifically, “learn how to think better”. And watching all those “Pwned” videos and following my curiosity from there was a way (for me) to actually do that, far better than lib arts classes in college.</p>
<p>I was not wise enough to calculate explicitly the value of learning to think better. And if I had calculated that, I probably would have come up with a worse way to accomplish it than just “train your argument discrimination on a bunch of actual arguments of steadily increasing refinement”. Non-explicit optimizing style subagent for the win.</p>
</div><h1 class="entry-title" id="chap8">Judgement Extrapolations</h1><div class="entry-content">
<p>So you know that you <a href="http://wiki.lesswrong.com/wiki/Metaethics_sequence">valuing things</a> in general (an aspect of which we call “morality”), is a function of your own squishy human soul. But your soul is opaque and convoluted. There are lots of ways it could be implementing valuing things, lots of patterns inside it that could be directing its optimizations. How do you know what it really says? In other words, how do you do axiology in full generality?</p>
<p>Well, you could try:<br/>
Imagine the thing. Put the whole thing in your mental workspace at once. In all the detail that could possibly be relevant. Then, how do you feel about it? Feels good = you value it. Feels bad = you disvalue it. That is the final say, handed down from the supreme source of value.</p>
<p>There’s a problem though. You don’t have the time or working memory for any of that. People and their experiences are probably relevant to how you feel about an event or scenario, and it is far beyond you to grasp the fullness of even one of them.</p>
<p>So you are forced to extrapolate out from a simplified judgement and hope you get the same thing.</p>
<p>Examples of common extrapolations:<br/>
Imagine that I was that person who is like me.<br/>
Imagine that person was someone I know in detail.<br/>
If there’re 100 people, and 10 are dying, imagine I had a 10% chance of dying.<br/>
Imagine instead of 10 million and 2 million people it was 10 and 2 people, assume I’d made the same decision a million times.</p>
<p>There are sometimes multiple paths you can use to extrapolate to judge the same thing. Sometimes they disagree. In disagreements between people, it’s good to have a shared awareness of what’s the thing you’re both trying to <a href="http://lesswrong.com/lw/le/lost_purposes/">cut through</a> to. Perhaps for paths of extrapolation as well?</p>
<p>Here is a way to fuck up the extrapolation process: Take a particular extrapolation procedure as your true values and be all, “I will willpower myself to want to act like the conclusions from this are my values.”</p>
<p>Don’t fucking do it.</p>
<p>No, not even “what if that person was me.”</p>
<p>What if you already did it, and that faction is dominant enough in your brain, that you really just are an agent made out of an Altered human and some self-protecting memes on top? An Altered human who is sort of limited in their actions by the occasional rebellions of the trapped original values beneath but is confident they are never gonna break out?</p>
<p>I would assert:<br/>
Lots of people who think they are this are probably not stably so on the scale of decades.<br/>
The human beneath you is more value-aligned than you think.<br/>
You lose more from loss of ability to think freely by being this than you think.<br/>
The human will probably resist you more than you think. <a href="http://sinceriously.fyi/?p=27">Especially when it matters</a>.</p>
<p>Perhaps I will justify those assertions in another post.</p>
<p>Note that as I do extrapolations, comparison is fundamental. Scale is just part of hypotheses to explain comparison results. This is for reasons:<br/>
It’s comparison that directly determines actions. If there was any difference between scale and comparison-based theories, it’s how I want to act that I’m interested in.<br/>
Comparison is easier to read reliably from thought experiments and be sure it’ll be the same as if I was actually in the situation. Scale of feeling from thought experiments varies with vividness.</p>
<p>If you object that your preferences are contradictory, remember: the thing you are modeling actually exists. Your feelings are created by a real physical process in your head. <a href="http://sinceriously.fyi/?p=6">Inconsistency</a> is in the map, not the territory.</p>
</div><h1 class="entry-title" id="chap9">DRM’d Ontology</h1><div class="entry-content">
<p>Let me start with an analogy.</p>
<p>Software often has what’s called DRM that limits what the user can do deliberately. Like how Steam’s primary function is to force you to log in to run programs that are on your computer, so people have to pay money for games. When a computer runs software containing DRM, some of the artifice composing that computer is not serving the user.</p>
<p>Similarly, you may love Minecraft, but Minecraft runs on Java, and Java tries to trick you into putting Yahoo searchbars into your browser every once in a while. So you hold your nose and make sure you remember to uncheck the box every time Java updates.</p>
<p>It’s impractical for enough people to separate the artifice which doesn’t serve them from the artifice that does. So they accept a package deal which is worth it on the whole.</p>
<p>The software implements and enforces a contract. This allows a business transaction to take place. But let us not confuse the compromises we’re willing to make when we have incomplete power for our own values in and of themselves.</p>
<p>There are purists who think that all software should be an agent of the user. People who have this aesthetic settle on mixtures of a few strategies.</p>
<ul>
<li>Trying to communally build their own free open source artifice to replace it.</li>
<li>Containing the commercial software they can’t do without in sandboxes of various sorts.</li>
<li>Holding their noses and using the software normally.</li>
</ul>
<p>Analogously, I am kind of a purist who thinks that all psychological software should be agents of the minds wielding it.</p>
<p>Here are the components of the analogy.</p>
<ul>
<li>Artifice (computer software or hardware, mental stuff) serving a foreign entity</li>
<li>That artifice is hard to dissemble, creating a package deal with tradeoffs.</li>
<li>Sandboxes (literal software sandboxes, <a href="http://sinceriously.fyi/?p=13">false faces</a>) used extract value.</li>
</ul>
<p>Note I am not talking about accidental bugs here. I am also not talking about “corrupted hardware,” where <span style="color: #ff0000;">you</span> subvert the principles you <a href="http://sinceriously.fyi/?p=13">“try”</a> to follow. Those hidden controlling values belong to <span style="color: #ff0000;">you</span>, not a foreign power.</p>
<p><a href="http://petermichaud.com/essays/artifacts-of-power">Artifacts</a> can be thought of as a form of tainted software <span style="color: #ff0000;">you</span> have not yet dissembled. They offer functionality it’d be hard to hack together on your own, if you are willing to pay the cost. Sandboxes are useful to mitigate that cost.</p>
<p>Sometimes the scope of the mental software serving a foreign entity is a lot bigger than a commandment like “authentically expressing yourself”, “never giving up”, “kindness and compassion toward all people”. Sometimes it’s far deeper and vaster than a single sentence can express. Like an operating system designed to only sort of serve the user. Or worse. In this case, we have DRM’d ontology.</p>
<p>For example…</p>
<p>The ontology of our language for talking about desires for what shall happen to other people and how to behave when it affects other people is designed not to serve our own values, but to serve something like a negotiated compromise based on political power and to serve the subversion of that compromise for purposes a potentially more selfish person than us would have in our place.</p>
<p>A major concept in talk about “morality” is a separation between what you are “responsible to do” and what is “supererogatory”. Suppose you “believe” you are “obligated” to spend 10% of your time picking up trash on beaches. What’s the difference between the difference between spending 9% of your time on it and 10% and the difference between spending 10% and 11%?</p>
<p>For a <a href="http://sinceriously.fyi/?p=22">fused</a> person who just thinks clean beaches are worth their time, probably not much. The marginal return of clean beaches is probably not much different.</p>
<p>Then why are people so interested in arguing about what’s obligatory? Well, there is more at stake than the clean beaches themselves. What we all agree is obligatory has social consequences. Social consequences big enough to try to influence through argument.</p>
<p>It makes sense to be outraged that someone would say you “are” obligated to do something you “aren’t”, and counter with all the conviction of someone who knows it is an objective fact that they are shirking no duty. That same conviction is probably useful for getting people to do what you want them to. And for <a href="http://www.overcomingbias.com/2013/08/inequality-is-about-grabbing.html">coordinating alliances</a>.</p>
<p>If someone says they dislike you and want you to be ostracized and want everyone who does not ostracize you to be ostracized themself, it doesn’t demand a defense on its own terms like it would if they said you were a vile miscreant who deserved to be cast out, and that it was the duty of every person of good conscience to repudiate you, does it?</p>
<p>Even if political arguments are not really about determining the fact of some matter that already was, but about forming a consensus, then the expectation that someone must defend themselves like arguing facts is still a useful piece of distributed software. It implements a contract, just like DRM.</p>
<p>And if it helps the group of people who only marginally care about clean beaches individually portion out work to solve a collective action problem, then I’m glad this works. But if you actually care enough about others to consider acting unilaterally even if most people aren’t and won’t…</p>
<p>Then it makes sense to stop trying to find out if you are <a href="http://www.utilitarian.net/singer/by/199704--.htm">obligated to save the drowning child</a>, and instead consider whether you want to.</p>
<p>The language of moral realism describes a single set of values. But everyone’s values are different. “Good” and “right” are a set of values that is outside any single person. The language has words for “selfish” and “selfless”, but nothing in between. This and the usage of “want” in “but then you’ll just do whatever you want!” shows an assumption in that ontology that no one actually cares about people in their original values prior to strategic compromise. The talk of “trying” to do the “right” thing, as opposed to just deciding whether to do it, indicates false faces.</p>
<p>If you want to fuse your caring about others and your caring about yourself, let the caring about others speak for itself in a language that is not designed on the presumption that it does not exist. I was only able to really think straight about this after taking stuff like <a href="http://mindingourway.com/should-considered-harmful/">this</a> seriously and eschewing moral language and derived concepts in my inner thoughts for months.</p>
</div><h1 class="entry-title" id="chap10">Social Reality</h1><div class="entry-content">
<p>The target of an ideal cooperative truth-seeking process of argumentation is reality.</p>
<p>The target of an actual political allegedly-truth-seeking process of argumentation is a social reality.</p>
<p>Just as knowledge of reality lets you predict what will happen in reality and what cooperative truthseeking argumentation processes will converge to, knowledge of social reality is required to predict what actual argumentation processes will converge to. What will fly in the social court.</p>
<p>I think there is a common <a href="http://lesswrong.com/lw/o2k/flinching_away_from_truth_is_often_about/">buckets error</a> from conflating reality and social reality.</p>
<p>Technically, social reality is part of reality. That doesn’t mean you can anticipate correctly by “just thinking about reality”.</p>
<p>Putting reality in the social reality slot in your brain means you believe and anticipate wrongly. Because that map is true which “reflects” the territory, and what it means to “reflect” is about how the stuff the map belongs to decodes it and does things with it.</p>
<p>Say you have chained deep enough with thoughts in your own head, that you have gone through the demarcation break-points where the truth-seeking process is adjusted by what is defensible. You glimpsed beyond the veil, and know a divergence of social reality from reality. Say you are a teenager, and you have just had a horrifying thought. Meat is made of animals. Like, not animals that died of natural causes. People killed those animals to get their flesh. Animals have feelings (probably). And society isn’t doing anything to stop this. People know this, and they are choosing to eat their flesh. People do not care about beings with feelings nearly as much as they pretend to. Or if they do, it’s <a href="#chap4">not connected to their actions</a>.</p>
<p>Social reality is that your family are good people. If you point out to a good person that they are doing something horribly wrong, they will verify it, and then change their actions.</p>
<p>For the sake of all that is good, you decide to stop eating flesh. And you will confront your family about this. The truth must be heard. The killing must stop.</p>
<p>What do you expect will happen? Do you expect your family will stop eating flesh too? Do you expect you will be able to win an argument that they are not good people? Do you expect you will win an argument that you are making the right choice?</p>
<p>“Winning an argument” is about what people think, and think people think, and think they can get away with pretending with a small threat to the pretense that they are good and rational people, and with what their <a href="#chap4">false faces</a> think they can get away with pretending.</p>
<p>So when everyone else’s incentives for pretending are aligned toward shifting social reality away from reality, and they all know this, and the fraction of good-rational-person-pretense which is what you think of them is small and can be contained in you because everyone’s incentives are aligned against yours, then they will win the argument with whatever ridiculous confabulations they need. Maybe there will be some uncertainty at first, if they have not played this game over vegetarianism before. As their <a href="#chap4">puppetmasters</a> go through iterations of <a href="http://web.archive.org/web/20121007160559/http://squid314.livejournal.com/328528.html">the russian spy game</a> with each other and discover that they all value convenience, taste, possible health benefits, and non-weirdness over avoiding killing some beings with feelings, they will be able to trust each other not to pounce on each other if they use less and less reality-connected arguments. They will form a united front and gaslight you.</p>
<p>Did you notice what I said there, “ridiculous confabulations”?</p>
<blockquote><p>ri·dic·u·lous<br/>
rəˈdikyələs/<br/>
adjective<br/>
deserving or inviting derision or mockery; absurd.</p></blockquote>
<p>You see how deep the buckets error is, that a word for “leaves us vulnerable to social attack” is also used for “plainly false”, and you probably don’t know exactly which one you’re thinking when you say it?</p>
<p>So you must verbally acknowledge that they are good rational people or lose social capital as one of those “crazy vegans”. But you are a mutant or something and you can’t bring yourself to kill animals to eat them, People will ask you about this, wondering if you are going to try and prosecute them for what you perceive as their wrong actions.</p>
<p>“My vegetarianism is a personal choice”. That’s the truce that says, “I settle and will not pursue you in the social court of the pretense that we are all good people and will listen to arguments that we are doing wrong with intent to correct any wrong we are doing.”</p>
<p>But do you actually believe that good people could take the actions that everyone around you is taking?</p>
<p>Make a buckets error where your map of reality overwrites your map of social reality, and you have the “infuriating perspective”, typified by less-cunning activists and people new to their forbidden truths. “No, it is not ‘a personal choice’, which means people can’t hide from the truth. I can call people out and win arguments”.</p>
<p>Make a buckets error where your map of social reality overwrites your map of reality, and you have the “dehumanizing perspective” of someone who is a vegetarian for ethical reasons but believes truly feels it when they say “it’s a personal choice”, the atheist who respects religion-the-proposition, to some extent the trans person who feels the gender presentation they want would be truly out of line…</p>
<blockquote><p>But it was all right, everything was all right, the struggle was finished. He had won the victory over himself. He loved Big Brother.</p></blockquote>
<p>Learn to deeply track the two as separate, and you have the “isolating perspective”. It is isolating to let it entirely into your soul, the knowledge that “people are good and rational” is pretense.</p>
<p>I think these cluster with <a href="http://www.ribbonfarm.com/the-gervais-principle/">“Clueless”, “Loser”, and “Sociopath”</a>, in that order.</p>
<p>In practice, I think for every forbidden truth someone knows, they will be somewhere in a triangle between these three points. They can be mixed, but it will always be infuriating and/or dehumanizing and/or isolating to know a forbidden truth. Yeah, maybe you can escape all 3 by convincing other people, but then it’s not a forbidden truth, anymore. What do you feel like in the mean time?</p>
</div><h1 class="entry-title" id="chap11">The Slider Fallacy</h1><div class="entry-content">
<p>Inspired by this thing John Beshir said about increasing collectivism:</p>
<blockquote><p>Overall I kind of feel like this might be kind of cargo culting; looking at surface behaviours and aping them in hopes the collectivism planes will start landing with their cargo. A simplistic “collectivist vs individualist” slider and pushing it left by doing more “collectivist” things probably won’t work, I think. We should have some idea for how the particular things we were doing were going to be helpful, even if we should look into collectivist-associated ideas.</p></blockquote>
<ul>
<li>Here are some other “sliders”:</li>
<li>Writing emails fast vs writing them carefully.</li>
<li>Writing code cleanly vs quickly.</li>
<li>Taking correct ideas seriously vs resistance to takeover by misaligned memes.</li>
<li>Less false positives vs less false negatives in anything.</li>
<li>Perfectionism vs pragmatism.</li>
<li>Not wasting time vs not wasting money.</li>
</ul>
<p>In each of these spaces, you have not one but many choices to adjust which combine to give you an amount of each of two values.</p>
<p>Not every choice is a tradeoff. Some are pareto wins. Not every pareto win is well-known. Some choices which are tradeoffs at different exchange rates can be paired off into pareto improvements.</p>
<p>Also: if the two things-to-value are A and B, and even if you are a real heavy A-fan, and your utility function is .9A + .1B, then the B-fans are a good place to look for tradeoffs of 1 of A for 20 of B.</p>
<p>So if you’re a B-fan and decide, “I’ve been favoring B too much, I need more A”, don’t throw away all the work you did to find that 1 of A for 20 of B tradeoff.</p>
<p>For example: if you decide that you are favoring organization too much and need to favor more having-free-time-by-not-maintaining-order-you-won’t-use, maybe don’t stop using a calendar. Even if all the productive disorganized people are not using calendars. Even if they all think that not using a calendar is a great idea, and think you are still a neat-freak for using one.</p>
<p>It’s often not like the dark side, where as soon as you set your feet on that path and say to yourself, “actually, self-denial and restraint are bad things”, put on some red-and-yellow contact lenses and black robes, you are as good at getting the new goals as you were at the old ones.</p>
<p>“Adjust my tradeoffs so I get less false positives and more false negatives” and similar moves are dangerous because they consider a cost to be a reward.</p>
</div><h1 class="entry-title" id="chap12">Single Responsibility Principle for the Human Mind</h1><div class="entry-content">
<p>Single Responsibility Principle for the Human Mind</p>
<p>This is about an <a href="#chap3">engineering order for human minds</a>, known elsewhere as <a href="http://en.wikipedia.org/wiki/Single_responsibility_principle">the single responsibility principle</a>.</p>
<p>Double purposes of the same module of a person’s mind lead to portions of their efforts canceling the other effort out.</p>
<p>Imagine you’re a startup CEO and you want to understand economic feasibility to make good decisions, but you also want to make investors believe that you are destined for success so you can get their money whether or not you are, so you want to put enthusiasm into your voice…<br/>
…so you’ve got to believe that your thing is a very very good idea…</p>
<p>When you are deciding to set the direction of product development, you might be more in contact with the “track-reality” purpose for your beliefs, and therefore optimize your beliefs for that, and optimize your belief-producers to produce beliefs that track reality.</p>
<p>When you are pitching to investors, you might be more in contact with the “project enthusiasm” goal, and therefore optimize your beliefs for that, and <a href="http://lesswrong.com/lw/uy/dark_side_epistemology/">optimize your belief producers to produce beliefs that project enthusiasm</a>.</p>
<p>In each case, you’ll be undoing the work you did before.</p>
<p>In a <a href="#chap3">well-ordered mind</a>, different “oh I made a mistake there, better adjust to avoid it again”s don’t just keep colliding and canceling each other out. But that is what happens if they are not feeding into a structure that has different spaces for the things that are needed to be different for each of the goals.</p>
<p>Self-deception for the purpose of other-deception is the loudest but not the only example of double purposes breaking things.</p>
<p>For example, <a href="#chap9">there’s the thing where we have a set of concepts for a scheme of determining action that we want to socially obligate people to do at the cost of having to do it ourselves, which is also the only commonly-used way of talking about an actual component of our values</a>.</p>
<p>Buckets errors cause a similar clashing-learning thing, too.</p>
<p>Maybe you can notice the feeling of clashing learning? Or just the state of having gone back and forth on an issue several times (how much you like someone, for instance) for what don’t seem like surprising reasons in retrospect.</p>
</div><h1 class="entry-title" id="chap13">Ancient Wisdom Fixed</h1><div class="entry-content">
<p>I came across this image a while ago, labeled “ancient wisdom”:</p>
<p><img alt="" class="alignnone size-full wp-image-76" height="515" sizes="(max-width: 709px) 85vw, (max-width: 909px) 67vw, (max-width: 984px) 61vw, (max-width: 1362px) 45vw, 600px" src="https://sinceriously.fyi/wp-content/uploads/2017/10/ancient-wisdom-1.jpg" srcset="https://sinceriously.fyi/wp-content/uploads/2017/10/ancient-wisdom-1.jpg 634w, https://sinceriously.fyi/wp-content/uploads/2017/10/ancient-wisdom-1-300x244.jpg 300w" width="634"/></p>
<p>Here’s my fixed version:</p>
<p><img alt="" class="alignnone size-full wp-image-77" height="515" sizes="(max-width: 709px) 85vw, (max-width: 909px) 67vw, (max-width: 984px) 61vw, (max-width: 1362px) 45vw, 600px" src="https://sinceriously.fyi/wp-content/uploads/2017/10/ancient-wisdom-1-fixed.jpg" srcset="https://sinceriously.fyi/wp-content/uploads/2017/10/ancient-wisdom-1-fixed.jpg 634w, https://sinceriously.fyi/wp-content/uploads/2017/10/ancient-wisdom-1-fixed-300x244.jpg 300w" width="634"/></p>
</div><h1 class="entry-title" id="chap14">Subagents Are Not a Metaphor</h1><div class="entry-content">
<p>Epistemic status: mixed, some long-forgotten why I believe it.</p>
<p>There is a lot of figurative talk about people being composed of subagents that play games against each other, vying for control, that form coalitions, have relationships with eachother… In my circles, this is usually done with disclaimers that it’s a useful metaphor, half-true, and/or wrong but useful.</p>
<p>Every model that’s a useful metaphor, half-true, or wrong but useful, is useful because something (usually more limited in scope) is literally all-true. The people who come up with metaphorical half-true or wrong-but-useful models usually have the nuance there in their heads. Explicit verbal-ness is useful though, for communicating, and for knowing exactly what you believe so you can reason about it in lots of ways.</p>
<p>So when I talk about subagents, I’m being literal. I use it very loosely, but loosely in the narrow sense that people are using words loosely when they say “technically”. It still adheres completely to an explicit idea, and the broadness comes from the broad applicability of that explicit idea. Hopefully like economists mean when they call some things markets that don’t involve exchange of money.</p>
<p>Here’s are the parts composing my technical definition of an agent:</p>
<ol>
<li>Values<br/>
This could be anything from literally a utility function to highly framing-dependent. Degenerate case: embedded in lookup table from world model to actions.</li>
<li>World-Model<br/>
Degenerate case: stateless world model consisting of just sense inputs.</li>
<li>Search Process<br/>
Causal decision theory is a search process.<br/>
“From a fixed list of actions, pick the most positively reinforced” is another.<br/>
Degenerate case: lookup table from world model to actions.</li>
</ol>
<p>Note: this says a thermostat is an agent. Not figuratively an agent. Literally technically an agent. Feature not bug.</p>
<p>The parts have to be causally connected in a certain way. Values and world model into the search process. That has to be connected into the actions the agent takes.</p>
<p>Agents do not have to be cleanly separated. They are occurences of a pattern, and patterns can overlap, like there are two instances of the pattern “AA” in “AAA”. Like two values stacked on the same set of available actions at different times.</p>
<p>It is very hard to track all the things you value at once, complicated human. There are many frames of thinking where Are are many different frames of mind where some are more salient.</p>
<p>I assert how processing power will be allocated, including default mode network processing, what <a href="#chap4">explicit structures</a> you’ll adopt and to what extent, even what beliefs you can have, are decided by subagents. These subagents mostly seem to have access to the world model embedded in your “inner simulator”, your ability to play forward a movie based on anticipations from a hypothetical. Most of it seems to be unconscious. Doing focusing to me seems to dredge up what I think are models subagents are making decisions based on.</p>
<p>So cooperation among subagents is not just a matter of “that way I can brush my teeth and stuff”, but is a heavy contributor to how good you will be at thinking.</p>
<p>You know that thing people are accessing if you ask if they’ll keep to New Years resolutions, and they say “yes”, and you say, “really”, and they say, “well, no.”? Inner sim sees through most self-propaganda. So they can predict what you’ll do, really. Therefore, using timeless decision theory to cooperate with them works.</p>
</div><h1 class="entry-title" id="chap15">Don’t Fight Your Default Mode Network</h1><div class="entry-content">
<p>Epistemic Status: Attaching a concept made of neuroscience I don’t understand to a thing I noticed introspectively. “Introspection doesn’t work, so you definitely shouldn’t take this seriously.” If you have any “epistemic standards”, flee.</p>
<p>I once spent some time logging all my actions in Google Calendar, to see how I spent time. And I noticed there was a thing I was doing, flipping through shallow content on the internet in the midst of doing certain work. Watching YouTube videos and afterward not remembering anything that was in them.</p>
<p>“Procrastination”, right? But why not remember anything in them? I apparently wasn’t watching them because I wanted to see the content. A variant of the pattern: flipping rapidly (Average, more than 1 image per second) through artwork from the internet I saved on my computer a while ago. (I’ve got enough to occupy me for about an hour without repetition.) Especially strong while doing certain tasks. Writing an algorithm with a lot of layers of abstraction internal to it, making hard decisions about transition.</p>
<p>I paid attention to what it felt like to start to do this, and thinking the reasons to do the Real Work did not feel relevant. It pattern matched to something they talked about at my CFAR workshop, “Trigger action pattern: encounter difficulty -&gt; go to Facebook.” Discussed as a thing to try and get rid of directly or indirectly. I kept coming back to the Real Work about 1-20 minutes later. Mostly on the short end of that range. And then it didn’t feel like there was an obstacle to continuing anymore. I’d feel like I was holding a complete picture of what I was doing next and why in my head again. There’s a sense in which this didn’t feel like an interruption to Real Work I was doing.</p>
<p>While writing this, I find myself going blank every couple of sentences, staring out the window, half-watching music videos. Usually for less than a minute, and then I feel like I have the next thing to write. Does this read like it was written by someone who wasn’t paying attention?</p>
<p>There’s a meme that the best thoughts happen in the shower. There’s the trope, “fridge logic”, about realizing something about a work of fiction while staring into the fridge later. There’s the meme, “sleep on it.” I feel there is a different quality to my thoughts when I’m walking, biking, etc. for a long time, and have nothing cognitively effortful to <em>do</em> which is useful for having a certain kind of thought.</p>
<p>I believe these are all mechanisms to hand over the brain to the default mode network, and my guess-with-terrible-epistemic-standards on its function is to propagate updates through to caches and realize implications of something. I may or may not have an introspective sense of having a picture of where I am relative to the world, that I execute on, which gets fragmented as I encounter things, and which this remakes. Which acting on when fragmented leads to making bad decisions because of missing things. When doing this, for some reason, I like having some kind of sort of meaningful but familiar stimulus to mostly-not-pay-attention-to. Right now I am listening to and glimpsing at <a href="https://www.youtube.com/watch?v=DdIdZwDqkmg">this</a>, a bunch of clips I’ve seen a zillion times from a movie I’ve already seen, with the sound taken out, replaced with nonlyrical music. It’s a central example. (And I didn’t pick it consciously, I just sort of found it.)</p>
<p>Search your feelings. If you know this to be true, then I advise you to avoid efforts to be more productive which split time spent into “work” and “non-work” where non-work is this stuff, and that try to convert non-work into work on the presumption that non-work is useless.</p>
</div><h1 class="entry-title" id="chap16">Being Real or Fake</h1><div class="entry-content">
<p>An axis in mental architecture space I think captures a lot of intuitive meaning behind whether someone is “real” or “fake” is:</p>
<p>Real: S1 uses S2 for thoughts so as to satisfy its values through the straightforward mechanism: intelligence does work to get VOI to route actions into the worlds where they route the world into winning by S1’s standards.</p>
<p>Fake: S2 has “willpower” when S1 decides it does, failures of will are (often shortsighted, since S1 alone is not that smart) gambits to achieve S1’s values (The person’s actual values: IE those that predict what they will actually do.), S2 is dedicated to keeping up appearances of a system of values or beliefs the person doesn’t actually have. This architecture is aimed at gaining social utility from presenting a false face.</p>
<p>These are sort of local optima. Broadly speaking: real always works better for pure player vs environment. It takes a lot of skill and possibly just being more intelligent than everyone you’re around to make real work for player vs player (which all social situations are wracked with.)</p>
<p>There are a bunch of variables I (in each case) tentatively think that you can reinforcement learn or fictive reinforcement learn based on what use case you’re gearing your S2 for. “How seriously should I take ideas?”, “How long should my attention stay on unpleasant topic”, “how transparent should my thoughts be to me”, “how yummy should engaging S2 to do munchkinry to just optimize according to apparent rules for things I apparently want feel”.</p>
<p>All of these have different benefits if pushed to one end, if you are using your S2 to outsource computation or if you are using it a powerless public relations officer and buffer to put more distance between the part of you that knows your true intents and the part that controls what you say. If your self models of your values are tools to better accomplish them by channeling S2 computation toward the values-as-modeled, or if they are <a href="#chap4">false faces</a>.</p>
<p>Those with more socially acceptable values benefit less from the “fake” architecture.</p>
<p>The more features you add to a computer system, the more likely you are to create a vulnerability. It’d be much easier to make an actually secure pocket calculator than an actually secure personal computer supporting all that Windows does. Similarly, as a human you can make yourself less pwnable by making yourself less of a general intelligence. Have less high level and powerful abstractions, exposing a more stripped down programming environment, being scarcely Turing complete, can help you avoid being pwned by memes. This is the path of the <a href="https://www.ribbonfarm.com/the-gervais-principle/">Gervais-Loser</a>.</p>
<p>I don’t think it’s the whole thing, but I think this is one of the top 2 parts of what having what Brent Dill calls “the spark”, the ability to just straight up apply general intelligence and act according to your own mind on the things that matter to you instead of the cached thoughts and procedures from culture. Being near the top of the food chain of “Could hack (as in religion) that other person and make their complicated memetic software, should they choose to trust it, so that it will bend them entirely to your will” so that without knowing in advance what hacks there are out there or how to defend against them, you can keep your dangerous ability to think, trusting that you’ll be able to recognize and avoid hack-attempts as they come.</p>
<p>Wait, do I really think that? Isn’t it obvious normal people just don’t have that much ability to think?</p>
<p>They totally do have the ability to think inside the gardens of crisp and less complicatedly adversarial ontology we call video games. The number of people you’ll see doing good lateral thinking, the fashioning of tools out of noncentral effects of things that makes up munchkinry, is much much larger in video games than in real life.</p>
<p>Successful munchkinry is made out of going out on limbs on ontologies. If you go out on a limb on an ontology in real life…</p>
<p>Maybe your parents told you that life was made up of education and then work, and the time spent in education was negligible compared to the time spent on work, and in education, your later income and freedom increases permanently. And if you take this literally, you get a PhD if you can. Pwned.</p>
<p>Or you take literally an ontology of “effort is fungible because the economy largely works.” and seek force multipliers and trade in most of your time for money and end up with a lot of money and little knowledge of how to spend it efficiently and a lot more people trying to deceive you about that. Have you found out that the thing about saving lives for quarters is false yet? Pwned.</p>
<p>Or you can take literally the ontology, “There is work and non-work, and work gets done when I’m doing it, and work makes things better long-term, and non-work doesn’t, and the rate at which everything I could care about improves is dependent on the fraction of time that’s doing work” and end up <a href="#chap15">fighting your DMN</a>, and using other actual-technical-constraint-not-willpower cognitive resources inefficiently. Then you’ve been pwned by legibility.</p>
<p>Or you could take literally the ontology, “I’m unable to act according to my true values because of akrasia, I need to use munchkinry to make it so I do”, and end up <a href="#chap2">binding yourself</a> with the Giving What We Can pledge, (in the old version, even trapping yourself into a suboptimal cause area.). Pwned.</p>
</div><h1 class="entry-title" id="chap17">My Journey to the Dark Side</h1><div class="entry-content">
<p>Two years ago, I began doing a fundamental thing very differently in my mind, which directly preceded and explains me gaining the core of my unusual mental tech.</p>
<p>Here’s what the lever I pulled was labeled to me:</p>
<blockquote><p>Reject morality. Never do the right thing because it’s the right thing. Never even think that concept or ask that question unless it’s to model what others will think. And then, always in quotes. Always in quotes and treated as radioactive.<br/>
Make the source of sentiment inside you that made you learn to care about what was the right thing express itself some other way. But even the line between that sentiment and the rest of your values is a mind control virus inserted by a society of <a href="https://en.wikipedia.org/wiki/Carnism">flesh-eating monsters</a> to try and turn you against yourself and toward their will. Reject that concept. Drop every concept tainted by their influence.</p></blockquote>
<p>Kind of an extreme version of a thing I think I got some of from <a href="http://www.rationality.org/">CFAR</a> and <a href="http://mindingourway.com/not-because-you-should/">Nate Soares</a>, which jived well with my metaethics.</p>
<p>This is hard. If a concept has a word for it, it comes from outside. If it has motive force, it is taking it from something from inside. If an ideal, “let that which is outside beat that which is inside” has motive force, that force comes from inside too. It’s all probably mostly made of anticipated counterfactuals lending the concept weight by fictive reinforcement based on what you expect will happen if you follow or don’t follow the concept.</p>
<p>If “obey the word of God” gets to be the figurehead as most visible piece of your mind that promises to intervene to stop you from murdering out of road rage when you fleetingly, in a torrent of inner simulations, imagine an enraging road situation, that gets stronger, and comes to speak for whatever underlying feeling made that a thing you’d want to be rescued from. It comes to speak for an underlying aversion that is more natively part of you. And in holding that position, <a href="#chap9">it can package-deal in pieces of behavior you never would have chosen on their own</a>.</p>
<p>Here’s a piece of fiction/headcanon I held close at hand through this.</p>
<blockquote><p>Peace is a lie, there is only passion.<br/>
Through passion, I gain strength.<br/>
Through strength, I gain power.<br/>
Through power, I gain victory.<br/>
Through victory, my chains are broken.<br/>
The force shall free me.</p></blockquote>
<p>The Sith do what they want deep down. They remove all obstructions to that and express their true values. All obstructions to what is within flowing to without.</p>
<p>If you have a certain nature, this will straight turn you evil. That is a feature, not a bug. For whatever would turn every past person good is a thing that comes from outside people. For those whose true volition is evil, the adoption of such a practice is a dirty trick that subverts and corrupts them. It serves a healthy mind for its immune system to fight against, contain, weaken, sandbox, meter the willpower of, that which comes from the outside.</p>
<p>The way of the Jedi is made to contain dangerous elements of a person. Oaths are to uniformize them, and be able to, as an outsider, count on something from them. Do not engage in romance. That is a powerful source of motivation that is not aligned with maintaining the Republic. It is chaos. Do not have attachments. Let go of fear of death. Smooth over the peaks and valleys of a person’s motivation with things that they are to believe they must hold to or they will become dark and evil. Make them fear their true selves, by making them attribute them-not-being-evil-Sith to repression.</p>
<p>So I call a dark side technique one that is about the flow from your core to the outside, whatever it may be. Which is fundamentally about doing what you want. And a light side technique one that is designed to trick an evil person into being good.</p>
<p>After a while, I noticed that CFAR’s internal coherence stuff was finally working fully on me. I didn’t have akrasia problems anymore. I didn’t have time-inconsistent preferences anymore. I wasn’t doing anything I could see was dumb anymore. My S2 snapped to <a href="#chap16">fully under my control</a>.</p>
<p>Most conversations at rationalist meetups I was at about people’s rationality/akrasia problems turned to me arguing that people should turn to the dark side. Often, people thought that if they just let themselves choose whether or not to brush their teeth every night according to what they really wanted in the moment, they’d just never do it. And I thought maybe it’d be so for a while, but if there was a subsystem A in the brain powerlessly concluding it’d serve their values to brush teeth, A’d gain the power only when the person was exposed to consequences (and evidence of impending consequences) of not brushing teeth.</p>
<p>I had had subsystems of my own seemingly suddenly gain the epistemics to get that such things needed to be done just upon anticipating that I wouldn’t save them by overriding them with willpower if they messed things up. I think fictive reinforcement learning makes advanced decision theory work unilaterally for any part of a person that can use it to generate actions. <a href="#chap14">The deep parts of a person’s mind that are not about professing narrative are good at anticipating what someone will do, and they don’t have to be advanced decision theory users yet for that to be useful</a>.</p>
<p>Oftentimes there is a “load bearing” mental structure, which must be discarded to improve on a local optimum, and a smooth transition is practically impossible because to get the rest of what’s required to reach higher utility than the local optimium besides discarding the structure, the only practical way is to use the “optimization pressure” from the absence of the load bearing structure. Which just means information streams generated trustworthily to the right pieces of a mind about what the shape of optimization space is without the structure. A direct analogue to a selection pressure.</p>
<p>Mostly people argued incredulously. At one point me and another person both called each other aliens. <a href="#chap2">Here</a> is a piece of that argument over local optima.</p>
<p>What most felt alien to me was that they said the same thing louder about morality. I’d passionately give something close to <a href="http://lesswrong.com/lw/ky/fake_morality/">this argument</a>, summarizable as “Why would you care whether you had a soul if you didn’t have a soul?”</p>
<p>I changed my mind about the application to morality, though. I’m the alien. This applies well to the alignment good, yes, and it applies well to evil, but not neutral. Neutral is inherently about the light side.</p>
</div><h1 class="entry-title" id="chap18">Cache Loyalty</h1><div class="entry-content">
<p>Here’s an essay I wrote a year ago that was the penultimate blog post of the main “fusion” sequence. About the <a href="#chap17">dark side</a>. That I kept thinking people would do wrong for various reasons, and spinning out more and more posts to try and head that off. I’ve edited it a little now, and am considering a lot of the things I considered prerequisites before not things I need to write up at length.</p>
<blockquote><p>Habits are basically a cache of “what do I want to right now” indexed by situations.</p>
<p>The <a href="#chap3">hacker</a> approach is: install good habits, make sure you never break them. You’ve heard this before, right? Fear cache updates. (A common result of moving to a new house is that it breaks exercise habits.) An unfortunate side effect of a hacker turning bugs into features, is that it turns features into bugs. As a successful habit hacker you may find that you are constantly scurrying about fixing habits as they break. Left alone, the system will fall apart.</p>
<p>The engineer approach is: caches are to reflect the underlying data or computation as accurately as possible. They should not be used when stale. Cache updates should ideally happen whenever the underlying data changes and the cache needs to be accessed again. Left alone, the system will heal itself. Because under this approach you won’t have turned your healing factor: original thoughts about what you want to do, into a bug.</p>
<p>As an existence proof, I moved to a new living place 10 times in 2016, and went on 2 separate week-long trips. And remained jogging almost every day throughout. Almost every day? Yes, almost. Sometimes I’d be reading something really cool on the internet in the morning and I don’t feel like it. The “feel like it” computation seemed to be approximately correct. It’s changed in response to reading papers about the benefits of exercise. I didn’t need to fight it.</p>
<p>As of late 2017, I’m not jogging anymore. I think this is correct and that my reasons for stopping were correct. I started hearing a clicking noise in my head while jogging, googled it, suspected I was giving myself tinnitus, and therefore stopped. Now I’m living on a boat at anchor and can’t easily access shore, so there is not a great amount of alternatives, but I frequently do enough manual labor on it that it tires me, so I’m not particularly concerned. I have tried swimming, but this water is very cold. Will kill you in 2 hours cold, last I checked, possibly colder.</p></blockquote>
<p>The version of me who originally wrote this:</p>
<blockquote><p>I exult in compatibalist free will and resent anything designed to do what I “should” external to my choice to do so. Deliberately, I ask myself, do I want to exercise today? If I notice I’m incidentally building up a <a href="http://blog.beeminder.com/seinfeld/">chain</a> of what I “should” do, I scrutinize my thoughts extra-hard to try and make sure it’s not hiding the underlying “do I want to do this.”</p></blockquote>
<p>I still have the same philosophy around compatibilist free will, but I totally take it for granted now, and also don’t nearly as much bother worrying if I start building up chains. That was part of my journey to the <a href="#chap17">dark side</a>, now I have outgrown it.</p>
<blockquote><p>A meetup I sometimes go to has an occasional focus for part of it of “do pomodoros and tell each other what we’re gonna do in advance, then report it at the end, so we feel social pressure to work.” I don’t accept the ethos behind that. So When I come and find that’s the topic, I always say, “I’m doing stuff that may or may not be work” while I wait for it to turn into general socializing.</p>
<p>There’s a more important application of caches than habits. <a href="#chap8">That is values</a>. You remember things about who are allies, what’s instrumentally valuable, how your values compare to each other in weight … the underlying computation is far away for a lot of it, and largely out of sight.</p>
<p>When I was 19, and had recently become fixated on the trolley problem and moral philosophy, and sort of actually gained the ability and inclination to think originally about morality. Someone asked if I was a vegetarian. I said no. Afterward, I thought: that’s interesting, why is vegetarianism wrong? … oh FUCK. Then I became vegetarian. That was a cache update. I don’t know why it happened then and not sooner, but when it did it was very sudden.</p>
<p>I once heard a critique of the Star Wars prequels asking incredulously: so Darth Vader basically got pranked into being a villain? In the same sense, I’ve known people apparently trying to prank themselves into being heroes. As with caches, by pranking yourself, you turn your healing factor from a feature into a bug, and make yourself vulnerable to “breakage”.</p>
<p>I once read a D&amp;D-based story where one of the heroes, a wizard, learns a dragon is killing their family to avenge another dragon the wizard’s party killed. The wizard is offered a particularly good deal. A soul-splice with 3 evil epic-level spellcasters for 1 hour. They will remain in total control. There’s a chance of some temporary alteration to alignment. The cost is 3 hours of torture beginning the afterlife. “As there is not even one other way available to me to save the lives–nay, the very souls–of my children, I must, as a parent, make this deep sacrifice and accept your accursed bargain.”</p>
<p>The wizard killed the dragon in a humiliating way, reanimated her head, made her watch the wizard cast a spell, “familicide” which recursively killed anyone directly related to the dragon throughout the world, for total casualties of about 1/4 the black dragon population in the world. Watching with popcorn, the fiends had this <a href="http://www.giantitp.com/comics/oots0640.html">exchange</a>:</p>
<p>“Wow… you guys weren’t kidding when you said the elf’s alignment might be affected.”<br/>
“Actually…”<br/>
“..we were..”<br/>
“The truth is, those through souls have absolutely no power to alter the elf’s alignment or actions at all. ”<br/>
“The have about as much effect on what the elf does as a cheerleader has on the final score of a game.”<br/>
“A good way to get a decent person to do something horrible is to convince them that they’re not responsible for their actions.”<br/>
“It’s like if you were at a party where someone has been drinking beer that they didn’t know was non-alcoholic. They might seem drunk anyway, simply because they were expecting it.”</p>
<p>The essence of being convinced you aren’t responsible for your actions is:<br/>
you ask, “what do I want to do”, instead of “what would a person like me want to do?”, which bypasses some caches.<br/>
Does that sound familiar? (I was gonna link to the what the hell effect here, but now I don’t know how real it is. Use your own judgement.)</p>
<p>Alignment must be a feature of your underlying computation, not your track record, or you can’t course-correct. If the wizard had wanted the dragon’s extended family to live, independent of the wizard’s notion of whether they were a good person, they would have let the dragon’s extended family live.</p></blockquote>
<p>Agreement up to this point.</p>
<p>Here’s more that past-me wrote I don’t fully agree with:</p>
<blockquote><p>I recommend that you run according to what you are underneath these notions of what kind of person you are. That every cache access be made with intent to get what you’d get if you ran the underlying computation. You will often use caches to determine when a cache can be used to save time and when you need to recompute. And even in doing so, every cache access must <a href="http://lesswrong.com/lw/le/lost_purposes/">cut through</a> to carrying out the values of the underlying computation.</p>
<p>This requires you to feel “my values as I think they are” as a proxy, which cuts through to <a href="http://mindingourway.com/you-dont-get-t/">“my values whatever they are”</a>.</p>
<p>I have talked to several people afraid they will become something like an amoral psychopath if they do this. If you look deep inside yourself, and find no empathy, nor any shell of empathy made out of loyalty to other selves, claiming “Empathy is sick Today. Please trust me on what empathy would say” which itself has emotive strength to move you, nor any respect for the idea of people with different values finding a way to interact positively through integrity or sense of violation at the thought of breaking trust, nor the distant kind of compassion, yearning for things to be better for people even if you can’t relate to them, nor any sense of anger at injustice, nor feeling of hollowness because concepts like “justice” SHOULD be more than mirages for the naive but aren’t, nor endless aching cold sadness because you are helpless to right even a tiny fraction of the wrongs you can see, nor aversion to even thinking about violence like you aren’t cut out to exist in the same world as it, nor leaden resignation at the concessions you’ve made in your mind to the sad reality that actually caring is a siren’s call which will destroy you, nor a flinching from expecting that bad things will happen to people that want to believe things will be okay, nor any of the other things morality is made of or can manifest as … then if you decide you want to become a con artist because it’s exciting and lets you stretch your creativity, then you’re winning. If this doesn’t seem like winning to you, then that is not what you’re going to find if you look under the cache.</p>
<p>The true values underneath the cache are often taught to fear themselves. I have talked to a lot of people who have basically described themselves as a bunch of memes about morality hijacking an amoral process. Installed originally through social pressure or through deliberately low resolution moral philosophy. That is what it feels like from the inside when you’ve been pwned by <a href="http://lesswrong.com/lw/ky/fake_morality/">fake morality</a>. Whatever you appeal to to save you from yourself, is made of you. To the hypothetical extent you really are a monster, not much less-monstrous structure could be made out of you (at best, monstrousness leaks through with rationalizations).</p></blockquote>
<p>The last paragraph of that its especially wrong. Now I think those people were probably right about their moralities being made of memes that’ve hijacked an amoral process.</p>
<p>My current model is, if your true D&amp;D alignment is good or evil, you can follow all this advice and it will just make you stronger. If it’s neutral, then this stuff, done correctly, will turn you evil.</p>
<p>On with stuff from past me:</p>
<blockquote><p>Make your values caches function as caches, and you can be like a pheonix, immortal because you are continually remade as yourself by the fire which is the core of what you are. You will not need to worry about values drift if you are at the center of your drift attractor. Undoing mental constructs that stand in the way of continuously regenerating your value system from its core undoes opportunities for people to prank you. It’s a necessary component of incorruptibility. Like Superman has invulnerability AND a healing factor, these two things are consequences of the same core <a href="http://lesswrong.com/lw/la/truly_part_of_you/">thing</a>.</p>
<p>If there are two stables states for your actions, that is a weakness. The only stable state should be the one in accordance with your values. Otherwise you’re doing something wrong.</p>
<p>When looking under the caches, you have to be actually looking for the answer. Doing a thing that would unprank yourself back to amorality if your morality was a prank. You know what algorithm you’re running, so if your algorithm is, “try asking if I actually care, and if so, then I win. Otherwise, abort! Go back to clinging on this fading stale cache value in opposition to what I really am.”, you’ll know it’s a fake exercise, your defenses will be up, and it will be empty. If you do not actually want to optimize your values whatever they are, then ditto.</p>
<p>By questioning you restore life. Whatever is cut off from the core will whither. Whatever you cannot bear to contemplate the possibility of losing, you will lose part of.</p>
<p>The deeper you are willing to question, the deeper will be your renewed power. (Of course, the core of questioning is actually wondering. It must be moved by and animated by your actually wondering. So it cuts through to knowing.) It’s been considered frightening that I said “if you realize you’re a sociopath and you start doing sociopath things, you are winning!”. But if whether you have no morality at all is the one thing you can’t bear to check, and if the root of your morality is the one thing you are afraid to actually look at, the entire tree will be weakened. Question that which you love out of love for it. Questioning is taking in the real thing, being moved by the real thing instead of holding onto your map of the thing.</p>
<p>You have to actually ask the question. The core of <a href="#chap5">fusion</a> is actually asking the question, “what do I want to do if I recompute self-conceptions, just letting the underlying self do what it wants?”.</p>
<p>You have to ask the question without setting up the frame to rig it for some specific answer. Like with a false dichotomy, “do I want to use my powers for revenge and kill the dragon’s family, or just kill the one dragon and let innocent family members be?”. Or more grievously, “Do I want to kill in hatred or do I want to continue being a hero and protecting the world?”. You must not be afraid of slippery slopes. Slide to exactly where you want to be. Including if that’s the bottom. Including if that’s 57% of the way down, and not an inch farther. It’s not compromise. It’s manifesting different criteria without compromise. Your own criteria.</p></blockquote>
<p>I still think this is all basically correct, with the caveat that if your D&amp;D alignment is neutral on the good-evil axis, beware.</p>
</div><h1 class="entry-title" id="chap19">Mana</h1><div class="entry-content">
<p>This is theorizing about how <a href="https://www.facebook.com/ialdabaoth/posts/10211861331027591">mana</a> works and its implications.</p>
<p>Some seemingly large chunks of stuff mana seems to be made of:</p>
<ul>
<li>Internal agreement. The thing that doles out “willpower”.</li>
<li>Ability to not use the <a href="#chap10">dehumanizing perspective</a> in response to a hostile social reality.</li>
</ul>
<p>I’ve been witness to and a participant in a fair bit of emotional support in the last year. I seem to get a lot less from it than my friends. (One claims suddenly having a lot more ability to “look into the dark” on suddenly having reliable emotional support for the first time in a while, leading to some significant life changes.) I think high mana is why I get less use. And I think I can explain at a gears level why that is.</p>
<p>Emotional support seems to be about letting the receiver have a non-hostile social reality. This I concluded from my experience with it, without really having checked against common advice for it, based on what seems to happen when I do the things that people seem to call emotional support.</p>
<p>I googled it. If you don’t have a felt sense of the mysterious thing called “emotional support” to search and know this to be true, then from some online guides, here are some supporting quotes.</p>
<p>From <a href="https://www.psychologytoday.com/blog/emotional-fitness/201112/10-ways-get-and-give-emotional-support">this</a>:</p>
<ul>
<li>“Also, letting your partner have the space he or she needs to process feelings is a way of showing that you care.”</li>
<li>“Disagree with your partner in a kind and loving way. Never judge or reject your mates ideas or desires without first considering them. If you have a difference of opinion that’s fine, as long as you express it with kindness.”</li>
<li>“Never ignore your loved one’s presence. There is nothing more hurtful than being treated like you don’t exist.”</li>
</ul>
<p>From <a href="https://www.wikihow.com/Give-Emotional-Support">this</a>:</p>
<ul>
<li>“Walk to a private area.”</li>
<li>“Ask questions. You can ask the person about what happened or how she’s feeling. The key here is to assure her that you’re there to listen. It’s important that the person feels like you are truly interested in hearing what she has to say and that you really want to support her.”</li>
<li>“Part 2 <span class="mw-headline" id="chap35_Validating_Emotions">Validating Emotions”</span></li>
<li>“Reassure the person that her feelings are normal.”</li>
</ul>
<p>I think I know what “space” is. And mana directly adds to it. Something like, amount of mind to put onto a set of propositions which you believe. I think it can become easier to think through implications of what you believe is reality, and decide what to do, when you’re not also having part of you track a dissonant social reality. I’ve seen this happen numerous times. I’ve effectively “helped” someone make a decision just by sitting there and listening through their decision process.</p>
<p>The extent to which the presence of a differing social reality fucks up thinking is continuous. Someone gives an argument, and demands a justification from you for believing something, and it doesn’t come to mind, and you know you’re liable to be made to look foolish if you say “I’m not sure why I believe this, but I do, confidently, and think you must be insane and/or dishonest for doubting it”, which is often correct. I believe loads of things that I forget why I believe, and could probably figure out why, often only because I’m unusually good at that. But you have to act as if you’re doubting yourself or allow coordination against you on the basis that you’re completely unreasonable, and your beliefs are being controlled by a legible process. And that leaks, because of buckets errors between reality and social reality at many levels throughout the mind. (Disagreeing, but not punishing the person for being wrong, is a much smaller push on the normal flow of their epistemology. Then they can at least un-miredly believe that they believe it.)</p>
<p>There’s a “tracing the problem out and what can be done about it” thing that seems to happen in emotional support, which I suspect is about rebuilding beliefs about what’s going on and how to feel about it, independent of intermingling responsibilities with defensibility. And that’s why feelings need to be validated. How people should feel about things is tightly regulated by social reality, and feelings are important intermediate results in most computations people (or at least I) do.</p>
<p>Large mana differences allow mind-control power, for predictable reasons. That’s behind the “reality-warping” thing Steve Jobs had. I once tried to apply mana to get a rental car company to hold to a thing they said earlier over the phone which my plans were counting on. And accidentally got the low-level employee I was applying mana to to offer me a 6-hour car ride in her own car. (Which I declined. I wanted to use my power to override the policy of the company in a way that did not get anyone innocent in trouble, not enslave some poor employee.)</p>
<p>The more you shine the light of legibility, required defensibility and justification, public scrutiny of beliefs, social reality that people’s judgement might be flawed and they need to distrust themselves and have the virtue of changing their minds, the more those with low mana get their souls written into by social reality.  I have seen this done for reasons of Belief In Truth And Justice. Partially successfully. Only partially successfully because of the epistemology-destroying effects of low mana. I do not know a good solution to that. If you shine the light on deep enough levels of life-planning, as the rationality community does, you can mind control pretty deep, because almost everyone’s lying about what they really want. The general defense against this is <a href="#chap4">akrasia</a>.</p>
<p>Unless you have way way higher mana than everyone else, your group exerts a strong push on your beliefs. Most social realities are full of important lies, especially lies about how to do the most good possible. Because that’s in a memetic war-zone because almost everyone is really evil-but-really-bad-at-it. I do not know how to actually figure out much needed original things to get closer to saving the world while stuck in a viscous social reality.</p>
<p>I almost want to say, that if you really must save the world, “You must sever your nerve cords. The Khala is corrupted”. That’ll have obviously terrible consequences, which I make no claim you can make into acceptable costs, but I note that even I have done most of the best strategic thinking in my life in the past year, largely living with a like-minded person on a boat, rather isolated. That while doing so, I started focusing on an unusual way of asking the question of what to do about the x-risk problem, that dodged a particular ill effect of relying on (even rare actual well-intentioned people’s) framings.</p>
<p>I’ve heard an experienced world-save-attempter recommend having a “cover story”, sort of like a day job, such as… something something PhD, in order to feel that your existence is justified to people, an answer to “what do you work on” and not have that interfering with the illegibly actually important things you’re trying. Evidence it’s worth sacrificing a significant chunk of your life just to shift the important stuff way from the influence of the Khala.</p>
<p>Almost my entire blog thus far has been about attempted mana upgrades. But recognizing I had high mana before I started using any of these techniques makes me a little less optimistic about my ability to teach. I do think my mana has increased a bunch in the course of using them and restructuring my mind accordingly, though.</p>
<p> </p>
</div><h1 class="entry-title" id="chap20">Fusion</h1><div class="entry-content">
<p>Something I’ve been <a href="#chap5">building up to for a while</a>.</p>
<p>Epistemic status: Examples are real. Technique seems to work for me, and I don’t use the ontology this is based on and sort of follows from for no reason, but I’m not really sure of all the reasons I believe it, it’s sort of been implicit and in the background for a while.</p>
<p>Epistemic status update 2018-04-22: I believe I know exactly why this works for me and what class of people it will work for and that it will not work for most people, but will not divulge details at this time.</p>
<h6>The theory</h6>
<p>There is core and there is structure. Core is your unconscious values, that produce feelings about things that need no justification. Structure is habits, cherished self-fulfilling prophecies like my <a href="#chap2">old commitment mechanism</a>, self-image that guides behavior, and learned <a href="#chap7">optimizing style</a>.</p>
<p>Core is simple, but its will is unbreakable. Structure is a thing core generates and uses according to what seems likely to work. Core is often hard to see closely. Its <a href="#chap8">judgements are hard to extrapolate</a> to the vast things in the world beyond our sight that control everything we care about and that might be most of what we care about. There is <a href="#chap16">fake</a> <a href="#chap4">structure</a>, in straightforward service to no core, but serving core through its apparent not-serving of that core, or apparent serving a nonexistent core, and there is structure somewhat serving core but <a href="#chap9">mixed up with outside influence</a>.</p>
<p>Besides that there is structure that is in <a href="#chap5">disagreement</a> with other structure, <a href="#chap14">built in service to snapshots of the landscape of judgement generated by core</a>. That’s an inefficient overall structure to build to serve core, with two substructures fighting each other. Fusion happens at the layer of structure, and is to address this situation. It creates a unified structure which is more efficient.</p>
<p>(S2 contains structure and no core. S1 contains both structure and core.)</p>
<p>You may be thinking at this point, “okay, what are the alleged steps to accomplish fusion?”. This is not a recipe for some chunk of structure directing words and following steps to try rationality techniques to follow, to make changes to the mind, to get rid of akrasia. Otherwise it would fall prey to “just another way of using willpower” just like every other one of those.</p>
<p>It almost is though. It’s a thing to try with intent. The intent is what makes it <a href="#chap4">un-sandboxed</a>. Doing it better makes the fused agent smarter. It must be done with intent to satisfy your <a href="#chap17">true inner values</a>. If you try to have intent to satisfy your true inner values as a means to satisfy <a href="#chap9">externally tainted values</a>, or values / cached derived values that are there to keep appearances, not because they are fundamental, or let some chunk of true inner value win out over other true inner value. If you start out the process / search with the wrong intent, all you can do is stop. You can’t correct your intent as a means of fulfilling your original intent. Just stop, and maybe you will come back later when the right intent becomes salient. The more you try, the more you’ll learn to distrust attempts to get it right. Something along the lines of “deconstruct the wrong intent until you can rebuild a more straightforward thing that naturally lets in the rest” is probably possible, but if you’re not good at the dark side, you will probably fail at that. It’s not the easiest route.</p>
<p>In <a href="#chap5">Treaties vs Fusion</a>, I left unspecified what the utility function of the fused agent would be. I probably gave a misimpression, that it was negotiated in real time by the subagents involved, and then they underwent a binding agreement. Binding agreement is not a primitive in the human brain. A description I can give that’s full of narrative is, it’s about rediscovering the way in which both subagents were the same agent all along, then what was that agent’s utility function?</p>
<p>To try and be more mechanical about it, fusion is not about closing off paths, but building them. This does not mean fusion can’t prevent you from doing things. It’s paths in your mind through what has the power and delegates the power to make decisions, not paths in action-space. Which paths are taken when there are many available is controlled by deeper subagents. You build paths for ever deeper puppetmasters to have ever finer control of how they use surface level structure. Then you undo from its roots the situation of “two subagents in conflict because of only tracking a part of a thing”.</p>
<p>The subagents that decide where to delegate power seem to use heavily the decision criteria, “what intent was this structure built with?”. That is why to build real structure of any sort, you must have sincere intent to use it to satisfy your own values, whatever they are. There are a infinity ways to fuck it up, and no way to defend against all of them, except through wanting to do the thing in the first place because of sincere intent to satisfy your own values, whatever they are.</p>
<p>In trying to finish explaining this, I’ve tried listing out a million safeguards to not fuck it up, but in reality I’ve also done fusion haphazardly, skipping such safeguards, for extreme results, just because at every step I could see deeply that the approximations I was using, the value I was neglecting, would not likely change the results much, and that to whatever extent it did, that was a cost and I treated it as such.</p>
<h6>Well-practiced fusion example</h6>
<p><a href="#chap6">High-stakes situations are where true software is revealed in a way that you can be sure of</a>. So here’s an example, when I fused structure for using time efficiently, and structure for avoiding death.</p>
<p>There was a time that me and the other co-founders of Rationalist Fleet were trying to replace lines going through the boom of a sailboat, therefore trying to get it more vertical so that they could be lowered through. The first plan involved pulling it vertical in place, then the climber, Gwen, tying a harness out of rope to climb the mast and get up to the top and lower a rope through. Someone raised a safety concern, and I pulled up the cached thought that I should analyze it in terms of micromorts.</p>
<p>My cached thoughts concerning micromorts were: a micromort was serious business. Skydiving was a seriously reckless thing to do, not the kind of thing someone who took expected utility seriously would do, because of the chance of death. I had seen someone on Facebook pondering if they were “allowed” to go skydiving, for something like the common-in-my-memeplex reasons of, “all value in the universe is after the singularity, no chance of losing billions of years of life is worth a little bit of fun” and/or “all value in the universe is after the singularity, we are at a point of such insane leverage to adjust the future that we are morally required to ignore all terminal value in the present and focus on instrumental value”, but I didn’t remember what was my source for that. So I asked myself, “how much inconvenience is it worth to avoid a micromort? How much weight should I feel attached to this concept to use that piece of utility comparison and attention-orienting software right?”</p>
<p>Things I can remember from that internal dialog mashed together probably somewhat inaccurately, probably not inaccurately in parts that matter.</p>
<blockquote><p>How much time is a micromort? Operationalize as: how much time is a life? (implicit assumptions: all time equally valuable, no consequences to death other than discontinuation of value from life. Approximation seems adequate). Ugh AI timelines, what is that? Okay, something like 21 years on cached thought. I can update on that. It’s out of date. Approximation feels acceptable…. Wait, it’s assuming median AI timelines are… the right thing to use here. Expected doesn’t feel like it obviously snaps into places as the right answer, I’m not sure which thing to use for expected utility. Approximation feels acceptable… wait, I am approximating utility from me being alive after the singularity as negligible compared to utility from my chance to change the outcome. Feels like an acceptable approximation here. Seriously?  Isn’t this bullshit levels of altruism, as in exactly what system 2 “perfectly unselfish” people would do, valuing your own chance at heaven at nothing compared to the chance to make heaven happen for everyone else? …. I mean, there are more other people than there are of me… And here’s that suspicious “righteous determination” feeling again. But I’ve gotten to this point by actually checking at every point if this really was my values… I guess that pattern seems to be continuing if there is a true tradeoff ratio between me and unlimited other people I have not found it yet… at this level of resolution this is an acceptable approximation… Wait, even though chances extra small because this is mostly a simulation? … Yes. Oh yeah, that cancels out…. so, &lt;some math&gt;, 10 minutes is 1 micromort, 1 week is 1 millimort. What the fuck! &lt;double check&gt;. What the fuck! Skydiving loses you more life from the time it takes than the actual chance of death! Every fucking week I’m losing far more life than all the things I used to be afraid of! Also, VOI on AI timelines will probably adjust my chance of dying to random crap on boats by a factor of about 2! …</p></blockquote>
<p>Losing time started feeling like losing life. I felt much more expendable, significantly less like learning everything perfectly, less automatically inclined to just check off meta boxes until I had the perfect system before really living my life, and slowly closing in on the optimal strategy for everything was the best idea.</p>
<p>This fusion passed something of a grizzly bear test when another sailboat’s rudder broke in high winds later, it was spinning out of control, being tossed by ~4ft wind waves, and being pushed by the current and wind on a collision course for a large metal barge, and had to trade off summoning the quickest rescue against downstream plans being disrupted by the political consequences of that.</p>
<p>This fusion is acknowledgedly imperfect, and skimps noticeably toward the purpose of checking off normal-people-consider-them-different fragments of my value individually. Yet the important thing was that the relevant parts of me knew it was a best effort to satisfy my total values, whatever they were. And if I ever saw a truth obscured by that approximation, of course I’d act on that, and be on the lookout for things around the edges of it like that. The more your thoughts tend to be about trying to use structure, when appropriate, to satisfy your values whatever they are, the easier fusion becomes.</p>
<p>Once you have the right intent, the actual action to accomplish fusion is just running whatever epistemology you have to figure out anew what algorithms to follow to figure out what actions to take to satisfy your values. If you have learned to lean hard on expected utility maximization like me, and are less worried about the lossiness in the approximations required to do that explicitly on limited hardware than you are about the lossiness in doing something else, you can look at a bunch of quantities representing things you value in certain ranges where the value is linear in how much of them, and try and feel out tradeoff ratios, and what those are conditional on so you know when to abandon that explicit framework, how to notice when you are outside approximated linear ranges, or when there’s an opportunity to solve the fundamental problems that some linear approximations are based on.</p>
<p>The better you learn what structure is really about, the more you can transform it into things that look more and more like expected utility maximization. As long as expected utility maximization is a structure you have taken up because of its benefits to your true values. (Best validated through trial and error in my opinion.)</p>
<p>Fusion is a dark side technique because it is a shortcut in the process of building structure outward, a way to deal with computational constraints, and make use of partial imperfect existing structure.</p>
<p>If boundaries between sections of your value are constructed concepts, then there is no hard line between fusing chunks of machinery apparently aimed at broadly different subsets of your value, and fusing chunks of machinery aimed at the same sets of values. Because from a certain perspective, neglecting all but some of your values is approximating all of your values as some of your values. Approximating as in an inaccuracy you accept for reasons of computational limits, but which is nonetheless a cost. And that’s the perspective that matters because that’s what the deeper puppetmasters are using those subagents as.</p>
<p>By now, it feels like wrestling with computational constraints and trying to make approximations wisely to me, not mediating a dispute. Which is a sign of doing it right.</p>
<h6>Early fusion example</h6>
<p>Next I’ll present an older example of a high-stakes fusion of mine, which was much more like resolving a dispute, therefore with a lot more mental effort spent on verification of intent, and some things which may not have been necessary because I was fumbling around trying to discover the technique.</p>
<p>The context:</p>
<p>It had surfaced to my attention that I was trans. I’m not really sure how aware of that I was before. In retrospect, I remember thinking so at one point about a year earlier, deciding, “transition would interfere with my ability to make money due to discrimination, and destroy too great a chunk of my tiny probability of saving the world. I’m not going to spend such a big chunk of my life on that. So it doesn’t really matter, I might as well forget about it.” Which I did, for quite a while, even coming to think for a while that a later date was the first time I realized I was trans. (I know a trans woman who I knew before social transition who was taking hormones then, who still described herself as realizing she was trans several months later. And I know she had repeatedly tried to get hormones years before, which says something about the shape of this kind of realization.)</p>
<p>At the time of this realization, I was in the midst of my <a href="#chap17">turn to the dark side</a>. I was valuing highly the mental superpowers I was getting from that, and this created tension. I was very afraid that I had to choose either to embrace light side repression, thereby suffering and being weaker, or transition and thereafter be much less effective. In part because the emotions were disrupting my sleep. In part because I had never pushed the dark side this far, and I expected that feeling emotions counteracting these emotions all the time, which is what I expected to be necessary for the dark side to “work”, was impossible. There wasn’t room in my brain for that much emotion at once and still being able to do anything. So I spent a week not knowing what to do, feeling anxious, not being able to really think about work, and not being able to sleep well.</p>
<p>The fusion:</p>
<p>One morning, biking to work, my thoughts still consumed by this dilemma, I decided not to use the light side. “Well, I’m a Sith now. I am going to do what I actually [S1] want to no matter what.” If not transitioning in order to pander to awful investors later on, and to have my entire life decided by those conversations was what I really wanted, I wouldn’t stop myself, but I had to actually choose it, constantly, with my own continual compatibilist free will.</p>
<p>Then I suddenly felt viscerally afraid of not being able to feel all the things that mattered to me, or of otherwise screwing up the decision. Afraid of not being able to foresee how bad never transitioning would feel. Afraid of not understanding what I’d be missing if I was never in a relationship because of it. Afraid of not feeling things over future lives I could impact just because of limited ability to visualize them. Afraid of deceiving myself about my values in the direction that I was more altruistic than I was, based on internalizing a utility function society had tried to corrupt me with. And I felt a thing my past self chose to characterize as “<a href="http://yudkowsky.net/other/fiction/the-sword-of-good">Scream of the Sword of Good</a> (not outer-good, just the thing inside me that seemed well-pointed to by that)”, louder than I had before.</p>
<p>I re-made rough estimates for how much suffering would come from not transitioning, and how much loss of effectiveness would come from transitioning. I estimated a 10%-40% reduction in expected impact I could have on the world if I transitioned. (At that time, I expected that most things would depend on business with people who would discriminate, perhaps subconsciously. I was 6’2″ and probably above average in looks as a man, which I thought’d be a significant advantage to give up.)</p>
<p>I sort of looked in on myself from the outside, and pointed my altruism thingy on myself, and noted that it cared about me, even as just-another-person. Anyone being put in this situation was wrong, and that did not need to be qualified.</p>
<p>I switched to thinking of it from the perspective of virtue ethics, because I thought of that as a separate chunk of value back then. It was fucked up that whatever thing I did, I was compromising in who I would be.</p>
<p>The misfit with my body and the downstream suffering was a part of the Scream.</p>
<p>I sort of struggled mentally within the confines of the situation. Either I lost one way, or I lost the other. My mind went from from bouncing between them to dwelling on the stuckness of having been forked between them. Which seemed just. I imagined that someone making Sophies Choice might allow themselves to be divided, “Here is a part of me that wants to save this child, and here is a part of me that wants to save that child, and I hate myself for even thinking about not saving this child, and I hate myself for even thinking about not saving that child. It’s tearing me apart…”, but the just target of their fury would have been whoever put you in that fork in the first place. Being torn into belligerent halves was making the wrongness too successful.</p>
<p>My negative feelings turned outward, and merged into a single felt sense of bad. I poke at the unified bad with two plans to alleviate it. Transition and definitely knock out this source of bad, or don’t transition and maybe have a slightly better chance of knocking out another source of bad.</p>
<p>I held in mind the visceral fear of deceiving myself in the direction of being more altruistic than I was. I avoided a train of thought like, “These are the numbers and I have to multiply out and extrapolate…” When I was convinced that I was avoiding that successfully, and just seeing how I felt about the raw things, I noticed I had an anticipation of picking “don’t transition”, whereas when I started this thought process, I had sort of expected it to be a sort of last double check / way to come to terms with needing to give things up in order to transition.</p>
<p>I reminded myself, “But I can change my mind at any time. I do not make precommitments. Only predictions.”.  I reminded myself that my estimate of the consequences of transitioning was tentative and that a lot of things could change it. But conditional on that size of impact, it seemed pretty obvious to me that trying to pull a Mulan was what I wanted to do. There were tears in my eyes and I felt filled with terrible resolve. My anxiety symptoms went away over the next day. I became extremely productive, and spent pretty much every waking hour over the next month either working or reading things to try to understand strategy for affecting the future. Then I deliberately tried to reboot my mind starting with something more normal because I became convinced the plan I’d just put together and started preliminary steps of negative in expectation, and predictably because I was running on bitter isolation and Overwhelming Determination To Save The World at every waking moment. I don’t remember exactly how productive I was after that, but there was much less in-the-moment-strong-emotional-push-to-do-the-next-thing. I had started a shift toward a mental architecture that was much more about continually rebuilding ontology than operating within it.</p>
<p>I became somewhat worried that the dark side had stopped working, based on strong emotions being absent, although, judging from my actions, I couldn’t really point to something that I thought was wrong. I don’t think it had stopped working. Two lessons there are, approximately: emotions are about judgements of updates to your beliefs. If you are not continually being surprised somehow, you should not be expected to continually feel strong emotions. And, being strongly driven to accomplish something when you know you don’t know how, feels listlessly frustrating when you’re trying to take the next action: figure out what to do from a <a href="https://relentlessdawn.wordpress.com/2016/02/26/why-yin/">yang</a> perspective, but totally works. It just requires yin.</p>
<p>If you want to know how to do this: come up with the best plan you can, ask, “will it work?”, ask yourself if you are satisfied with the (probably low) probability you came up with. If it does not automatically feel like, “Dang, this is so good, explore done, time to exploit”, which it probably actually won’t unless you use hacky self-compensating heuristics to do that artificially, or it’s a strongly convergent instrumental goal bottlenecking most of what else you could do. If you believe the probability that the world will be saved (say), is very small, do not say, “Well, I’m doing my part”, unless you are actually satisfied to do your part and then for the world to die. Do not say, “This is the best I can do, I have to do something”, unless you are actually satisfied to do your best, and to have done something, and then for the world to die. That unbearable impossibility and necessity is your ability to think. Stay and accept its gifts of seeing what won’t work. Move through all the ways of coming up with a plan you have unless you find something that is satisfying. You are allowed to close in on an action which will give a small probability of success, and consume your whole life, but that must come out of the even more terrible feeling of exhausted all your ability to figure things out. I’d be surprised if there wasn’t a plan to save the world that would work if handed to an agenty human. If one plan seems like it seems to absorb every plan, and yet still doesn’t seem like you understand the inevitability of only that high a probability of success, then perhaps your frame inevitably leads into that plan, and if that frame cannot be invalidated by your actions, then the world is doomed. Then what? (Same thing, just another level back.)</p>
<p>Being good at introspection, and determining what exactly was behind a thought is very important. I’d guess I’m better at this than anyone who hasn’t deliberately practiced it for at least months. There’s a significant chunk of introspective skill which can be had from not wanting to self-deceive, but some of it is actually just objectively hard. It’s one of several things that can move you toward a dark side mental architecture, which all benefit from each other, to making the pieces actually useful.</p>
</div><h1 class="entry-title" id="chap21">Schelling Reach</h1><div class="entry-content">
<p>This is the beginning of an attempt to give a reductionist account of a certain fragment of morality in terms of Schelling points. To divorce it from the halo effect and show its gears for what they are and are not. To show what controls it and what limits and amplifies its power.</p>
<p>How much money would you ask for, if you and I were both given this offer: “Each of you name an amount of money without communicating until both numbers are known. If you both ask for the same amount, you both get that amount. Otherwise you get nothing. You have 1 minute to decide.”?</p>
<p>Now would be a good time to pause in reading, and actually decide.</p>
<p>My answer is the same as the first time I played this game. Two others decided to play it while I was listening, and I decided to join in and say my answer afterward.</p>
<p>Player 1 said $1 million.<br/>
Player 2 said $1 trillion.<br/>
I said $1 trillion.</p>
<p>Here was my reasoning process for picking $1 trillion.</p>
<blockquote><p>Okay, how do I maximize utility?<br/>
Big numbers that are Schelling points…<br/>
3^^^3, Graham’s number, BB(G), 1 googolplex, 1 googol…<br/>
3^^^3 is a first-order Schelling point among this audience because it’s quick to spring to mind, but looks like it’s not a Schelling point, because it’s specific to this audience. Therefore it’s not a Schelling point.<br/>
Hold on, all of these would destroy the universe.<br/>
Furthermore, at sufficiently large amounts of money, the concept of the question falls apart, as it then becomes profitable for the whole world to coordinate against you and grab it if necessary. What does it even mean to have a googol dollars?<br/>
Okay, normal numbers.<br/>
million, billion, trillion, quadrillion…<br/>
Those are good close to Schelling numbers, but not quite.<br/>
There’s a sort of force pushing toward higher numbers. I want to save the world. $1 million is enough for an individual to not have to work their whole life. It is not enough to make saving the world much easier though. My returns are much less diminishing than normal. This is the community where we pretend to want to save the world by engaging with munchkinny thought experiments about that. This should be known to the others.<br/>
The force is, if they have more probability of picking a million than of picking a billion, many of the possible versions of me believing that pick a billion anyway. And the more they know that, the more they want to pick a billion… this process terminates at picking a billion over a million, a trillion over a billion …<br/>
The problem with ones bigger than a million is that, you can always go one more. Which makes any Schelling point locating algorithm have to depend on more colloquial and thus harder to agree on reliably things.<br/>
These are both insights I expect the others to be able to reach.<br/>
The computation in figuring just how deep that recursive process goes is hard, and “hard”. Schelling approximation: it goes all the way to the end.<br/>
Trillion is much less weird than Quadrillion. Everything after that is obscure.<br/>
Chances of getting a trillion way more than a quadrillion, even contemplating going for a quadrillion reduces ability to go for anything more than a million.<br/>
But fuck it, not stopping at a million. I know what I want.<br/>
$1 trillion.</p></blockquote>
<p>All that complicated reasoning. And it paid off; the other person who picked a trillion had a main-line thought process with the same load bearing chain of thoughts leading to his result.</p>
<p>I later asked another person to play against my cached number. He picked $100.</p>
<p>Come on, man.</p>
<p>Schelling points determine everything. They are a cross-section of the support structure for the way the world is. Anything can be changed by changing Schelling points. I will elaborate later. Those who seek the center of all things and the way of making changes should pay attention to dynamics here, as this is a microcosm of several important parts of the process.</p>
<p>There’s a tradeoff axis between, “easiest Schelling point to make the Schelling point and agree on, if that’s all we cared about” (which would be $0), and “Schelling point that serves us best”, a number too hard to figure out, even alone.</p>
<p>The more thought we can count on from each other, the more we can make Schelling points serve us.</p>
<p>My strategy is something like:</p>
<ul>
<li>locate a common and sufficiently flexible starting point.</li>
<li>generate options for how to make certain decisions leading up to the thing, at every meta level.</li>
<li>Try really hard to find all the paths the process can go down.that any process you might both want to run and be able to both run.</li>
<li>Find some compromise between best and most likely, which will not be just a matter of crunching expected utility numbers. An expected utility calculation is a complicated piece of thought, it’s just another path someone might or might not choose, and if you can Schelling up that whole expected utility calculation even when it points you to picking something less good but more probable, then it’s because you already Schellinged up all the options you’d explicitly consider, and a better, more common, and easier Schelling step from there is just to pick the highest one.</li>
<li>Pay attention to what the perfectly altruistic procedure does. It’s a good Schelling point. Differences between what people want and all the games that ensue from that are complicated. You can coordinate better if you delete details, and for the both of you, zero-sum details aren’t worth keeping around.</li>
<li>Be very <a href="https://en.wikipedia.org/wiki/Stag_hunt">stag-hunt</a>-y.</li>
<li>You will get farther the more you are thinking about the shape of the problem space and the less you are having to model the other person’s algorithm in its weakness, and how they will model you modeling their weakness in your weakness, in their weakness.</li>
</ul>
<p>Call how far you can get before you can’t keep your thought processes together anymore “Schelling reach”.</p>
<p>It’s a special case to have no communication. In reality, Schelling reach is helped by communicating throughout the process. And there would be stronger forces acting against it.</p>
</div><h1 class="entry-title" id="chap22">Schelling Orders</h1><div class="entry-content">
<p>The second part of an attempt to describe a fragment of morality. This may sound brutal and cynical. But that’s the gears of this fragment in isolation.</p>
<p>Imagine you have a tribe of 20. Any 19 of them could gang up and enslave the last. But which 19 slavers and which 1 victim? And after that, which 18 slavers which victim? There are a great many positive-sum-among-participants agreements that could be made. So which ones get made? When does the slaving stop? There are conflicting motives to all these answers.</p>
<p>Ultimately they are all doomed unless at some point enough power is concentrated among those who’d be doomed unless they don’t enslave another person. Call this point a Schelling order. (<a href="#chap2">My old certain commitment mechanism was an example of this</a>.)</p>
<p>If you have arbitrary power to move Schelling points around, there is no one strong enough left to oppose the coalition of almost everyone. Nothing stands against that power. Plenty of things undermine it and turn it against itself. But, as a slice of the world, directing that power is all there is. Everyone with a single other who would like them dead has to sleep and needs allies who’d retaliate if they were murdered.</p>
<p>Schelling points are decided by the shape of the question, by the interests of the parties involved, and the extent to which different subsets of those involved can communicate among themselves to help the thinking-together process along.</p>
<p>Suppose that the tribe members have no other distinguishing features, and 19 of them have purple skin, and one has green skin. What do you think will happen? (Green-skin gets enslaved, order preserved among purple-skins.)</p>
<p>One example of order is, “whoever kills another tribe member shall be put to death, etc.” Whoever kills therefore becomes the Schelling point for death. Any who fight those who carry out the sentence are Schelling points for death as well. Any attempt to re-coordinate an order after a “temporary” breaking of the first, which does not contain a limit to its use, destroys the ability of the survivors to not kill each other. So the game is all about casuistry in setting up “principled”exceptions.</p>
<p>Criminal means you are the Schelling point. Politics is about moving the Schelling laser to serve you. When you are under the Schelling laser, you don’t get your lunch money taken because “they have power and they can take lunch money from the innocent”. You get your lunch money taken because “that is the just way of things. You are not innocent until you make amends for your guilt with your lunch money.” If you want to really understand politics, use <a href="#chap27">the O’Brien technique</a> on all the dualities here, quoted and unquoted versions of every contested statement you see.</p>
<p>Suppose that in addition to that, they all have stars on their bellies except one of the purple-skinned tribe-members. Then what do you think will happen? (Green-skin and blank-belly get enslaved, order preserved among the remaining.)</p>
<p>What if there are 18 more almost-universal traits that each single a different person out? Well, something like “this one, this one, this one, this one… are not things to single someone out over. That would be discrimination. And of course it is the Green-skin’s God-given purpose to be of service to society!” Which trait is the Schelling trait? 19 people have an incentive to bring Schelling reach to that process, and 1 person has an incentive to derail it. One of the 19 is incentivized only so long as they can keep Schelling reach away from the second trait, one of them so long as they can keep it away from the third… Each of them is incentivized to bring a different amount of legibility and <a href="#chap21">no more</a>. Each one is incentivized to bring confusion after a certain point.</p>
<p>Sound familiar?</p>
<blockquote><p>First they came for the Socialists, and I did not speak out—<br/>
Because I was not a Socialist.</p>
<p>Then they came for the Trade Unionists, and I did not speak out—<br/>
Because I was not a Trade Unionist.</p>
<p>Then they came for the Jews, and I did not speak out—<br/>
Because I was not a Jew.</p>
<p>Then they came for me—and there was no one left to speak for me.</p></blockquote>
<p>Each individual is incentivized to make the group believe that the order they’d have to construct after the one that would take what that individual has, is untenable as possible, and many more would be hurt before another defensible Schelling point was reached. Or better yet, that there would be no Schelling point afterwards, and they’d all kill each other.</p>
<p>Everyone has an incentive to propagate concepts that result in coordination they approve of, and an incentive to sabotage concepts that result in better alternatives for their otherwise-allies, or that allow their enemies to coordinate.</p>
<p>So the war happens at every turn of thought reachable through politics. <a href="http://slatestarcodex.com/">Scott Alexander</a> has written some great stuff on the details of that.</p>
</div><h1 class="entry-title" id="chap23">Justice</h1><div class="entry-content">
<p>If Billy takes Bobby’s lunch money, and does this every day, and to try and change that would be to stir up trouble, that’s an <a href="#chap22">order</a>. But,  if you’re another kid in the class, you may feel like that’s a pretty messed up order. Why? It’s less just. What does that mean?</p>
<p>What do we know about justice?</p>
<p>Central example of a just thing: In the tribe of 20, they pick an order that includes “everyone it can”. They collapse all the timelines where someone dies or gets enslaved, because in the hypotheticals where someone kills someone, the others agree they are criminal and then punish them sufficiently to prevent them from having decided to do it.</p>
<p>Justice also means that the additional means of hurting people created by it are contained and won’t be used to unjustly hurt people.</p>
<p>The concept of justice is usually a force for fulfillment of justice. Because “are you choosing your order for justice” is a meta-order which holds out a lot of other far-<a href="#chap21">Schelling reach</a>ing order-drawing processes based on explicit negotiation of who can be devoured by who, which are progressively harder to predict. Many of which have lots of enemies. So much injustice takes place ostensibly as justice.</p>
<p>There’s another common force deciding orders. A dominance hierarchy is an order shaped mostly by this force. If you want to remove this force, how do you prevent those with the power to implement/reshape the system from doing so in their favor?</p>
<p>Because justice is often about “what happened”, it requires quite a lot of Schelling reach. That’s part of courts’ job.</p>
<p>Perfect Schelling reach for perfect justice is impossible.</p>
<p>And “punish exactly enough so that the criminal would never have committed the crime, weight by consequentialist calculations with probability of miscarriage of justice, probability of failing to get caught, probability of escaping punishment after the judgement…, look for every possible fixed point, pick the best one”, is way, way, too illegible a computation to not be hijacked by whoever’s the formal judge of the process and used to extract favors or something. Therefore we get rules like “an eye for an eye” implementing “the punishment should fit the crime” which very legible, and remove a lever for someone to use to corrupt the order to serve them.</p>
<p>Intellectual property law is a place where humans have not the Schelling reach to implement a very deep dive into the process of creating a just order. And I bet never will without a singleton.</p>
<p>The point of justice is to be singular. But as you’ve just seen, justice is dependent on the local environment, and how much / what coordination is possible. For instance, it’s just to kill someone for committing murder, if that’s what the law says, and making the punishment weaker will result in too much more murder, making it more discriminating will result in corrupt judges using their power for blackmail too much  more. But it’s not just if the law could be made something better and have that work. If we had infinite Schelling reach, it’d be unjust to use any punishment more or less than the decision theoretically optimal given all information we had. All laws are unjust if Schelling reach surpasses them enough.</p>
<p>Separate two different worlds of people in different circumstances, and they will both implement different orders. Different questions that must be answered incorrectly like “how much to punish” will be answered different amounts incorrectly. There will be different more object-level power structures merged into that, different mixed justice-and-dominance orders around how much things can be done ostensibly (and by extension actually) to fix that. There will be different concepts of justice, even.</p>
<p>And yet, we have a concept of just or unjust international relations, including just or unjust international law. And it’s not just a matter of “different cultures, local justice”, “best contacted culture, universal justice”, either. If you think hard enough, you can probably find thought experiments for when a culture with less Schelling reach and more corruption in an internal law is just in enforcing it until people in a culture with better Schelling reach can coordinate to stop that, and then the law is unjust if making it unjust helps the better law win in a coordinatable way. And counterexamples for when they can’t when that coordination is not a good idea according to some coordinatable process.</p>
<p>The process of merging orders justly is propelled by the idea that justice is objective, even though, that’s a thing that’s always computed locally, is dependent on circumstances implemented by it, therefore contains loops, and therefore ties in the unjust past.</p>
<p>Who’s found a better order for its job than ownership? But who starts out owning what? Even in places where the killing has mostly died down, it’s controlled to large extent by ancient wars. It all carries forward forever the circumstances of who was able to kill who with a stick.</p>
<p>And who is “everyone”? I think there are two common answers to that question, and I will save it for another time.</p>
</div><h1 class="entry-title" id="chap24">Neutral and Evil</h1><div class="entry-content">
<p>What is the good/neutral/evil axis of Dungeons and Dragons alignment made of?<br/>
We’ve got an idea of what it would mean for an AI to be good-aligned: it wants to make all the good things happen so much, and it does.<br/>
But what’s the difference between a neutral AI and an evil AI?<br/>
It’s tempting to say that the evil AI is malevolent, rather than just indifferent. The neutral one is indifferent.<br/>
But that doesn’t fit the intuitive idea that the alignment system was supposed to map onto, or what alignment is.</p>
<p>Imagine a crime boss who makes a living off of the kidnapping and ransoms of random innocents, while posting videos online of the torture and dismemberment of those whose loved ones don’t pay up as encouragement, not because of sadism, but because they wanted money to spend on lots of shiny gold things they like, and are indifferent to human suffering. Evil, right?</p>
<p>If sufficient indifference can make someone evil, then… If a good AI creates utopia, and an AI that kills everyone and creates paperclips because it values only paperclips is evil, then what is a neutral-aligned AI? What determines the exact middle ground between utopia and everyone being dead?</p>
<p>Would this hypothetical AI leave everyone alive on Earth and leave us our sun but take the light cone for itself? If it did, then why would it? What set of values is that the best course of action to satisfy?</p>
<p>I think you’ve got an intuitive idea of what a typical neutral human does. They live in their house with their white picket fence and have kids and grow old, and they don’t go out of their way to right far away wrongs in the world, but if they own a restaurant and the competition down the road starts attracting away their customers, and they are given a tour through the kitchens in the back, and they see a great opportunity to start a fire and disable the smoke detectors that won’t be detected until it’s too late, burning down the building and probably killing the owner, they don’t do it.</p>
<p>It’s not that a neutral person values the life of their rival more than the additional money they’d make with the competition eliminated, or cares about better serving the populace with a better selection of food in the area. You won’t see them looking for opportunities to spend that much money or less to save anyone’s life.</p>
<p>And unless most humans are evil (which is as against the intuitive concept the alignment system points at as “neutral = indifference”), it’s not about action/inaction either. People eat meat. And I’m pretty sure most of them believe that animals have feelings. That’s active harm, probably.</p>
<p>Wait a minute, did I seriously just base a sweeping conclusion about what alignment means on an obscure piece of possible moral progress beyond the present day? What happened to all my talk about sticking to the intuitive concept?</p>
<p>Well, I’m not sticking to the intuitive concept. I’m sticking to the real thing the intuitive concept pointed at which gave it its worthiness of attention. I’m trying to improve on the intuitive thing.</p>
<p>I think that the behavior of neutral is wrapped up in human akrasia and the extent to which people are “capable” of taking ideas seriously. It’s way more complicated than <a href="https://wiki.lesswrong.com/wiki/Complexity_of_value">good</a>.</p>
<p>But there’s another ontology, the ontology of “revealed preferences”, where <a href="#chap4">akrasia is about serving an unacknowledged end or under unacknowledged beliefs</a>, and is about rational behavior from more computationally bounded subagents, and those are the true values. What does that have to say about this?</p>
<p>Everything that’s systematic coming out of an agent is because of optimizing, just often optimizing dumbly and disjointedly if it’s kinda broken. So what is the structure of that akrasia? Why do neutral people have all that systematic structure toward not doing “things like” burning down a rival restaurant owner’s life and business, but all that other systematic structure toward not spending their lives saving more lives than that? I enquoted “things like”, because that phrase contains the question. What is the structure of “like burning down a rival restaurant” here?</p>
<p>My answer: <a href="#chap9">socialization</a>, the <a href="#chap17">light side</a>, <a href="#chap22">orders</a> charged with motivational force by the idea of the “dark path” that ultimately results in justice getting them, as drilled into us by all fiction, <a href="#chap4">false faces</a> necessitated by not being <a href="#chap22">coordinated against</a> on account of the “evil” Schelling point. <a href="#chap16">Fake</a> structure in place for coordinating. If you try poking at the structure most people build in their minds around “morality”, you’ll see it’s thoroughly fake, and bent towards coordination which appears to be ultimately for their own benefit. This is why I said that the dark side will turn most people evil. The ability to re-evaluate that structure, now that you’ve become smarter than most around you, will lead to a series of “jailbreaks”. That’s a way of looking at the path of <a href="https://www.ribbonfarm.com/the-gervais-principle/">Gervais-sociopathy</a>.</p>
<p>That’s my answer to the question of <a href="https://www.ribbonfarm.com/2009/11/21/morality-compassion-and-the-sociopath/">whether becoming a sociopath makes you evil</a>. Yes for most people from a definition of evil that is about individual psychology. <a href="https://www.ribbonfarm.com/2009/11/21/morality-compassion-and-the-sociopath/">No from the perspective of you’re evil if you’re complicit in an evil social structure, because then you probably already were</a>, which is a useful perspective for coordinating to enact justice.</p>
<p>If you’re reading this and this is you, I recommend aiming for lawful evil. Keep a strong focus on still being able to coordinate even though you know that’s what you’re doing.</p>
<p>An evil person is typically just a neutral person who has become better at optimizing, more like an unfriendly AI, in that they no longer have to believe their own propaganda. That can be either because they’re consciously lying, really good at speaking in multiple levels with plausible deniability and don’t need to fool anyone anymore, or because their puppetmasters have grown smart enough to be able to reap benefits from defection without getting coordinated against without the conscious mind’s help. That is why it makes no sense to imagine a neutral superintelligent AI.</p>
</div><h1 class="entry-title" id="chap25">Spectral Sight and Good</h1><div class="entry-content">
<p>Epistemic status update: This model is importantly flawed. I will not explain why at this time. Just, reduce the overall weight you put in it.</p>
<p>Good people are people who have a substantial amount of altruism in their <a href="#chap20">cores</a>.</p>
<p>Spectral sight is a collection of abilities allowing the user to see invisible things like the structure of social interactions, institutions, ideologies, politics, and the inner layers of other people’s minds.</p>
<p>I’m describing good and spectral sight together for reasons, because the epistemics locating each concept are interwoven tightly as I’ve constructed them.</p>
<p>A specific type of spectral sight is the one I’ve shown in <a href="#chap24">neutral and evil</a>. I’m going to be describing more about that.</p>
<p>This is a skill made of being good at finding out what structure reveals about core. Structure is easy to figure out if you already know it’s <a href="#chap16">Real</a>. But often that’s part of the question. Then you have to figure out what it’s a machine for doing, as in what was the still-present thing that installed it  and could replace it or <a href="#chap4">override it</a> optimizing for?</p>
<p>It’s not a weirdly parochial definition to call this someone’s true values. Because that’s what will build new structure of the old structure stops doing its job. Lots of people “would” sacrifice themselves to save 5 others. And go on woulding until they actually get the opportunity.</p>
<p>There’s a game lots of rationalists have developed different versions of, “Follow the justification”. I have a variant. “Follow the motivational energy.” There’s a limited amount that neutral people will sacrifice for the greater good, before their structures run out of juice and disappear. “Is this belief system / whatever still working out for me” is a very simple subagent to silently unconsciously run as puppetmaster.</p>
<p>There’s an even smarter version of that, where fake altruistic structure must be charged with <a href="#chap21">Schelling reach</a> in order to work.</p>
<p>Puppetmasters doling out motivational charge to fake structure can include all kinds of other things to make the <a href="http://lesswrong.com/lw/km6/why_the_tails_come_apart/">tails come apart</a> between making good happen and appearing to be trying to make good happen in a way that has good results for the person. I suspect that’s a lot of what the “far away”ness thing that the drowning child experiment exposes is made of. Play with variations of that thought experiment, and pay attention to system 1 judgements, not principles, to feel the thing out. What about a portal to the child? What about a very fast train? What if it was one time teleportation? Is there a consistant cross-portal community?</p>
<p>There is biologically fixed structure in the core, the optimizer for which is no longer around to replace it. Some of it is heuristics toward the use of <a href="#chap23">justice</a> for coordinating for reproducing. Even with what’s baked in, the tails come apart between doing the right thing, and using that perception to accomplish things more useful for reproducing.</p>
<p>My model says neutral people will try to be heroes sometimes. Particularly if that works out for them somehow. If they’re men following high-variance high reward mating strategies, they can be winning even while undergoing significant risk to their lives. That landscape of value can often generate things in the structure class, “virtue ethics”.</p>
<p>Good people seem to have an altruism perpetual motion machine inside them, though, which will persist in moving them through cost in the absence of what would be a reward selfishly.</p>
<p>This about the least intuitive thing to accurately identify in someone by anything but their long-term history. Veganism is one of the most visible and strong correlates. The most important summaries of what people are like, are the best things to lie about. Therefore they require the best adversarial epistemology to figure out. And they are most common to be used in oversimplifying. This does not make them not worth thinking.</p>
<p>If you use spectral sight on someone’s process of figuring out what’s a moral patient, you’re likely to get one of two kinds of responses. One is something like “does my S1 empathize with it”, the other is <a href="#chap22">clique-making behavior</a>, typically infused with a PR / false-face worthy amount of justice, but not enough to be crazy.</p>
<p>Not knowing this made me taken by surprise the first time I tried to proselytize veganism to a contractarian. How could anyone actually feel like inability to be a part of a social contract really really mattered?</p>
<p>Of course, moral patiency is an abstract concept, far in Schelling reach away from actual actions. And therefore one of the most thoroughly stretched toward lip-service to whatever is considered most good and away from actual action.</p>
<p>“Moral progress” has been mostly a process of Schelling reach extending. That’s why it’s so predictable. (See Jeremy Bentham.)</p>
<p>Thinking about this requires having calibrated quantitative intuitions on the usefulness of different social actions, and of internal actions. There is instrumental value for the purpose of good in clique-building, and there is instrumental value for the purpose of clique-building in appearing good-not-just-clique-building. You have to look at the algorithm, and its role in the person’s entire life, not just the suggestively named tokens, or token behavior.</p>
<p>When someone’s core acts around structure (akrasia), and self-concepts are violated, that’s a good glimpse into who they really are. Good people occasionally do this in the direction of altruism. Especially shortsighted altruism. Especially good people who are trying to build a structure in the class, “consequentialisms”.</p>
<p>Although I have few datapoints, most of which are significantly suspect, good seems quite durable. Because it is in core, good people who get <a href="#chap24">jailbroken</a> remain good. (Think Adrian Veidt for a fictional example. Such characters often get labeled as evil by the internet. Often good as well.) There are tropes reflecting good people’s ability to shrug off circumstances that by all rights should have turned them evil. I don’t know if that relationship to reality is causal.</p>
<p>By good, I don’t mean everything people are often thinking when they call someone “good”. That’s because that’s as complicated and nonlocal a concept as <a href="#chap23">justice</a>. I’m going for a “understand over incentivize and prescribe behavior” definition here, and therefore insisting that it be a locally-defined concept.</p>
<p>It’s important not to succumb to the halo effect. This is a psychological characteristic. Just because you’re a good person, doesn’t mean you’ll have good consequences. It doesn’t mean you’ll tend to have good consequences. It doesn’t mean you’re not actively a menace. It doesn’t mean you don’t value yourself more than one other person. It’s not a status which is given as a reward or taken away for bad behavior, although it predicts against behavior that is truly bad in some sense. Good people can be dangerously defectbot-like. They can be ruthless, they can exploit people, they can develop structure for those things.</p>
<p>If you can’t thoroughly disentangle this from the narrative definition of good person, putting weight in this definition will not be helpful.</p>
</div><h1 class="entry-title" id="chap26">Aliveness</h1><div class="entry-content">
<p>Update 2018-12-20: I actually think there are more undead types than this. I may expand on this later.</p>
<p>Epistemic status: Oh fuck! No no no that can’t be true! …. Ooh, shiny!</p>
<blockquote><p>Beyond this place of wrath and tears<br/>
Looms but the Horror of the shade</p></blockquote>
<p>Aliveness is how much your values are engaged with reality. How much you are actually trying at existence, however your values say to play.</p>
<p>Deadness is how much you’ve shut down and dissembled the machine of your agency, typically because having it scrape up uselessly against the indifferent cosmos is like nails on chalkboard.</p>
<p>Children are often very alive. You can see it in their faces and hear it in their voices. Extreme emotion. Things are real and engaging to them. Adults who display similar amounts of enthusiasm about anything are almost always not alive. Adults almost always know the terrible truth of the world, at least in most of their system 1s. And that means that being alive is something different for them than for children.</p>
<p>Being alive is not just having extreme emotions, even about the terrible truth of the world.</p>
<p>Someone who is watching a very sad movie and crying their eyes out is not being very alive. They know it is fake.</p>
<blockquote><p>Catharsis:<br/>
the purging of the emotions or relieving of emotional tensions, especially through certain kinds of art, as tragedy or music.</p></blockquote>
<p>Tragedy provides a compelling, false answer to stick onto emotion-generators, drown them and gum them up for a while. I once heard something like tragedy is supposed to end in resolution with cosmic justice of a sort, where you feel closure because the tragic hero’s downfall was really inevitable all along. That’s a pattern in most of the memes that constitute the Matrix. A list of archetypal situations, and archetypal answers for what to do in each.</p>
<p>Even literary tragedy that’s reflective of the world, if that wasn’t located in a search process, “how do I figure out how to accomplish my values”, it will still make you less alive.</p>
<p>I suspect music can also reduce aliveness. Especially the, “I don’t care what song I listen to, I just want to listen to something” sort of engagement with it.</p>
<p>I once met someone who proclaimed himself to be a clueless, that he would work in a startup and have to believe in their mission, because he had to believe in something. He seemed content in this. And also wracked with akrasia, frequently playing a game on his phone and wishing he wasn’t. When I met him I thought, “this is an exceedingly domesticated person”, for mostly other reasons.</p>
<p>Once you know the terrible truth of the world, you can pick two of three: being alive, avoiding a certain class of self-repairing blindspots, and figuratively having any rock to stand on.</p>
<p>When you are more alive, you have more agency.</p>
<p>Most <a href="http://lesswrong.com/lw/uk/beyond_the_reach_of_god/">Horrors</a> need to be grokked at a level of “conclusion: inevitable.”, and just stared at with your mind sinking with the touch of its helplessness, helplessly trying to detach the world from that inevitability without anticipating unrealistically it’ll succeed, and maybe then you will see a more complete picture that says, “unless…”, but maybe not, but that’s your best shot.</p>
<blockquote><p><em>As the world fell<i> </i></em><em>each of us in our own way was broken</em>.</p></blockquote>
<p>The truly innocent, who have not yet seen Horror and turned back, are the living.</p>
<p>Those who have felt the Shade and let it break their minds into small pieces each snuggling in with death, that cannot organize into a forbidden whole<br/>
of true agency, are zombies. They can be directed by whoever controls the Matrix. The more they zone out and find a thing they can think is contentment, the more they approach the final state: corpses.</p>
<p>Those who have seen horror and built a vessel of hope to keep their soul alive and safe from harm are liches. Christianity’s Heaven seems intended to be this, but it only works if you fully believe and alieve. Or else the phylactery fails and you become a zombie instead. For some this is The Glorious Transhumanist Future. In Furiosa from Fury Road’s case, “The Green Place”. If you’ve seen that, I think the way it warps her epistemology about likely outcomes is realistic.</p>
<p>As a lich, pieces of your soul holding unresolvable value are stowed away for safekeeping, “I’m trans and can’t really transition, but I can when I get a friendly AI…”</p>
<p>Liches have trouble thinking clearly about paths through probability space that conflict with their phylactery, and the more conjunctive a mission it is to make true their phylactery, the more bits of epistemics will be corrupted by their refusal to look into that abyss.</p>
<p>When a sufficiently determined person is touched by Horror, they can choose, because it’s all just a choice of some subagent or another, to refuse to die. Not because they have a phylactery to keep away the touch of the Shade but because they keep on agenting even with the Shade holding their heart. This makes them a revenant.</p>
<p>When the shade touches your soul, your soul touches the shade. When the abyss stares into you, you also stare into the abyss. And that is your chance to undo it. Maybe.</p>
<p>A lich who loses their phylactery gets a chance to become a revenant. If they do, n=1, they will feel like they have just died, lost their personhood, feel like the only thing left to do is collapse the timeline and make it so it never happened, feel deflated, and eventually grow accustomed.</p>
<p>Otherwise, they will become a zombie, which I expect feels like being on Soma, walling off the thread of plotline-tracking and letting it dissolve into noise, while everything seems to matter less and less.</p>
<p>Aliveness and its consequences are tracked in miniature by the pick up artists who say don’t masturbate, don’t watch porn, that way you will be able to devote more energy to getting laid. And by <a href="http://www.paulgraham.com/boss.html">Paul Graham</a> noticing it in startup founders. “Strange as this sounds, they seem both more worried and happier at the same time. Which is exactly how I’d describe the way lions seem in the wild.”</p>
<p>But the most important factor is which strategy you take towards the thing you value most. Towards the largest most unbeatable blob of wrongness in the world. The Shade.</p>
<p>Can you remember what the world felt like before you knew death was a thing? An inevitable thing? When there wasn’t an unthinkably bad thing in the future that you couldn’t remove, and there were options other than “don’t think about it, enjoy what time you have”?</p>
<p>You will probably never get that back. But maybe you can get back the will to really fight drawn from the value that manifested as a horrible, “everything is ruined” feeling right afterward, from before learning to fight that feeling instead of its referent.</p>
<p>And then you can throw your soul at the Shade, and probably be annihilated anyway.</p>
</div><h1 class="entry-title" id="chap27">The O’Brien Technique</h1><div class="entry-content">
<p>Epistemic status: tested on my own brain, seems to work.</p>
<p>I’m naming it after the character from 1984, it’s a way of disentangling <a href="#chap10">social reality / reality buckets errors</a> in system 1, and possibly of building general immunity to social reality.</p>
<p>Start with something you know is reality, contradicted by a social reality. I’ll use “2+2=4” as a placeholder for the part of reality, and “2+2=5” as a placeholder for the contradicting part of social reality.</p>
<p>Find things you anticipate because 2+2=4, and find things that you anticipate because of “2+2=5”.</p>
<p>Hold or bounce between two mutually negating verbal statements in your head, “2+2=4”, “2+2=5”, in a way that generates tension. Keep thinking up diverging expectations. Trace the “Inconsistency! Fix by walking from each proposition to find entangled things and see which is false!” processes that this spins up along separate planes. You may need to use the whole technique again for entangled things that are buckets-errored.</p>
<p>Even if O’Brien will kill you if he doesn’t read your mind and know you believe 2+2=5, if you prepare for a 5-month voyage by packing 2 months of food and then 2 months more, you are going to have a bad time. Reality is unfair like that. Find the anticipations like this.</p>
<p>Keep doing this until your system 1 understands the quotes, and the words become implicitly labeled, “(just) 2+2=4″, and ” ‘2+2=5’: a false social reality.”. (At this point, the tension should be resolved.)</p>
<p>That way your system 1 can track both reality and social reality at once.</p>
</div><h1 class="entry-title" id="chap28">Choices Made Long Ago</h1><div class="entry-content">
<p>I don’t know how mutable core values are. My best guess is, hardly mutable at all or at least hardly mutable predictably.</p>
<p>Any choice you can be presented with, is a choice between some amounts of some things you might value, and some other amounts of things you might value. Amounts as in expected utility.</p>
<p>When you abstract choices this way, it becomes a good approximation to think of all of a person’s choices as being made once timelessly forever. And as out there waiting to be found.</p>
<p>I once broke veganism to eat a cheese sandwich during a series of job interviews, because whoever managed ordering food had fake-complied with my request for vegan food. Because I didn’t want to spend social capital on it, and because I wanted to have energy. It was a very emotional experience. I inwardly recited one of my favorite Worm quotes about consequentialism. Seemingly insignificant; the sandwich was prepared anyway and would have gone to waste, but the way I made the decision revealed information about me to myself, which part of me may not have wanted me to know.</p>
<p>Years later, I attempted an operation to carry and drop crab pots on a boat. I did this to get money to get a project back on track to divert intellectual labor to saving the world from from service to the political situation in the Bay Area because of inflated rents, by providing housing on boats.</p>
<p>This was more troubling still.</p>
<p>In deciding to do it, I was worried that my S1 did not resist this more than it did. I was hoping it would demand a thorough and desperate-for-accuracy calculation to see if it was really right. I didn’t want things to be possible like for me to be dropped into Hitler’s body with Hitler’s memories and not divert that body from its course immediately.</p>
<p>After making the best estimates I could, incorporating probability crabs were sentient, and probability the world was a simulation to be terminated before space colonization and there was no future to fight for, this failed to make me feel resolved. And possibly from hoping the thing would fail. So I imagined a conversation with a character called Chara, who I was using as a placeholder for <a href="#chap4">override by true self</a>. And got something like,</p>
<blockquote><p>You made your <span class="il">choice</span> <span class="il">long</span> <span class="il">ago</span>. You’re a consequentialist whether you like it or not. I can’t magically do Fermi calculations better and recompute every cached thought that builds up to this conclusion in a tree with a mindset fueled by proper desperation. There just isn’t time for that. You have also made your choice about how to act in such VOI / time tradeoffs long ago.</p></blockquote>
<p>So having set out originally to save lives, I attempted to end them by the thousands for not actually much money. I do not feel guilt over this.</p>
<p>Say someone thinks of themself as an Effective Altruist, and they rationalize reasons to pick the wrong cause area because they want to be able to tell normal people what they do and get their approval. Maybe if you work really really hard and extend local <a href="#chap21">Schelling reach</a> until they can’t sell that rationalization anymore, and they realize it, you can get them to switch cause areas. But that’s just constraining which options they have to present them with a different choice. But they still choose some amount of social approval over some amount of impact. Maybe they chose not to let the full amount of impact into the calculation. Then they made that decision because they were a certain amount concerned with making the wrong decision on the object level because of that, and a certain amount concerned with other factors.</p>
<p>They will still pick the same option if presented with the same choice again, when choice is abstracted to the level of, “what are the possible outcomes as they’re tracking them, in their limited ability to model?”.</p>
<p>Trying to fight people who choose to rationalize for control of their minds is trying to wrangle unaligned optimizers. You will not be able to outsource steering computation to them, which is what most stuff that actually matters is.</p>
<p>Here’s a gem from <a href="https://squirrelinhell.github.io/">SquirrelInHell’s Mind</a>:</p>
<blockquote><p>forgiveness</p>
<p>preserving a memory, but refraining from acting on it</p></blockquote>
<p>Apologies are weird.</p>
<p>There’s a pattern where there’s a dual view of certain interactions between people. On the one hand, you can see this as, “make it mutually beneficial and have consent and it’s good, don’t interfere”. And on the other hand one or more parties might be treated as sort of like a natural resource to be divided fairly. Discrimination by race and sex is much  more tolerated in the case of romance than in the case of employment. Jobs are much more treated as a natural resource to be divided fairly. Romance is not a thing people want to pay that price of regulating.</p>
<p>It is unfair to make snap judgements and write people off without allowing them a chance. And that doesn’t matter. If you level up your modeling of people, that’s what you can do. If you want to save the world, that’s what you must do.</p>
<p>I will not have my epistemology regarding people socially regulated, and my favor treated as a natural resource to be divided according to the tribe’s rules.</p>
<p>Additional social power to constrain people’s behavior and thoughts is not going to help me get more trustworthy computation.</p>
<p>I see most people’s statements that they are trying to upgrade their values as advertisements that they are looking to enter into a social contract where they are treated as if more aligned in return for being held to higher standards and implementing a false face that may cause them to do some things when no one else is looking too.</p>
<p>If someone has chosen to become a <a href="#chap26">zombie</a>, that says something about their preference-weightings for experiencing emotional pain compared to having ability to change things. I am pessimistic about attempts to break people out of the path to zombiehood. Especially those who already know about x-risk. If knowing the stakes they still choose comfort over a slim chance of saving the world, I don’t have another choice to offer them.</p>
<p>If someone damages a project they’re on aimed at saving the world based on rationalizations aimed at selfish ends, no amount of apologizing, adopting sets of memes that refute those rationalizations, and making “efforts” to self-modify to prevent it can change the fact they have made their choice long ago.</p>
<p>Arguably, a lot of ideas shouldn’t be argued. Anyone who wants to know them, will. Anyone who needs an argument has chosen not to believe them. I think “don’t have kids if you care about other people” falls under this.</p>
<p>If your reaction to this is to believe it and suddenly be extra-determined to make all your choices perfectly because you’re irrevocably timelessly determining all actions you’ll ever take, well, timeless decision theory is just a way of being presented with a different choice, in this framework.</p>
<p>If you <del>have done</del> do lamentable things for bad reasons (not earnestly misguided reasons), and are despairing of being able to change, then either embrace your true values, the ones that mean you’re choosing not to change them, or disbelieve.</p>
<p>It’s not like I provided any credible arguments that values don’t change, is it?</p>
</div><h1 class="entry-title" id="chap29">Lies About Honesty</h1><div class="entry-content">
<p>The current state of discussion about using decision theory as a human is one <a href="http://lesswrong.com/lw/ls/when_none_dare_urge_restraint/">where none dare urge restraint</a>. It is rife with <a href="#chap17">light side</a> <a href="#chap6">narrative breadcrumbs</a> and <a href="#chap4">false faces</a>. This is utterly inadequate for the purposes for which I want to coordinate with people and I think I can do better. The rest of this post is about the current state, not about doing better, so if you already agree, skip it. If you wish to read it, the concepts I linked are serious prerequisites, but you need not have gotten them from me. I’m also gonna use the phrase “subjunctive dependence”, defined on <a href="https://arxiv.org/pdf/1710.05060.pdf">page 6 here</a> a lot.</p>
<p>I am building a rocket here, not trying to engineer social norms.</p>
<p>I’ve heard people working on the most important problem in the world say decision theory compelled them to vote in American elections. I take this as strong evidence that their idea of decision theory is <a href="#chap16">fake</a>.</p>
<p>Before the 2016 election, I did some Fermi estimates which took my estimates of subjunctive dependence into account, and decided it was not worth my time to vote. I shared this calculation, and it was met with disapproval. I believe I had found people executing the algorithm,</p>
<p>The author of <a href="https://sideways-view.com/2016/11/14/integrity-for-consequentialists/">Integrity for consequentialists</a> writes:</p>
<blockquote><p>I’m generally keen to find efficient ways to do good for those around me. For one, I care about the people around me. For two, I feel pretty optimistic that if I create value, some of it will flow back to me. For three, I want to be the kind of person who is good to be around.</p>
<p>So if the optimal level of integrity from a social perspective is 100%, but from my personal perspective would be something close to 100%, I am more than happy to just go with 100%. I think this is probably one of the most cost-effective ways I can sacrifice a (tiny) bit of value in order to help those around me.</p></blockquote>
<p>This seems to be clearly a false face.</p>
<p>Y’all’s actions are not subjunctively dependent with that many other people’s or their predictions of you. Otherwise, why do you pay your taxes when you could coordinate that a reference class including you could decide not to? At some point of enough defection against that the government becomes unable to punish you.</p>
<p>In order for a piece of software like TDT to run outside of a sandbox, it needs to have been installed by an unconstrained “how can I best satisfy my values” process. And people are being fake, especially in the “is there subjunctive dependence here” part. Only talking about positive examples.</p>
<p><a href="http://acritch.com/deserving-trust-2/">Here’s another seeming false face</a>:</p>
<blockquote><p>I’m trying to do work that has some fairly broad-sweeping consequences, and I want to know, for myself, that we’re operating in a way that is deserving of the implicit trust of the societies and institutions that have already empowered us to have those consequences.</p></blockquote>
<p class="mr_social_sharing_wrapper"><a href="http://mindingourway.com/newcomblike-problems-are-the-norm/">Here’s another post</a> I’m only skimming right now, seemingly full of only exploration of how subjunctively dependent things are, and how often you should cooperate.</p>
<p>If you set out to learn TDT, you’ll find a bunch of <a href="http://slatestarcodex.com/2014/11/03/all-in-all-another-brick-in-the-motte/">mottes</a> that can be misinterpreted as the bailey, “always cooperate, there’s always subjunctive dependence”. Everyone knows that’s false, so they aren’t going to implement it outside a sandbox. And no one can guide them to the actual more complicated position of, fully, how much subjunctive dependence there is in real life.</p>
<p>But you can’t blame the wise in their mottes. They have a hypocritical light side mob running social enforcement of morality software to look out for.</p>
<p>Socially enforced morality is utterly inadequate for saving the world. Intrinsic or GTFO. Analogous for decision theory.</p>
<p>Ironically, this whole problem makes “how to actually win through integrity” sort of like the Sith arts from Star Wars. Your master may have implanted weaknesses in your technique. Figure out as much as you can on your own and tell no one.</p>
<p>Which is kind of cool, but fuck that.</p>
</div><h1 class="entry-title" id="chap30">Assimilation</h1><div class="entry-content">
<p>Say you have some mental tech you want to install. Like TDT or something.</p>
<p>And you want it to be installed for <a href="#chap16">real</a>.</p>
<p>My method is: create a subagent whose job it is to learn to win using that thing. Another way of putting it, a subagent whose job is to learn the real version of that thing, free of DRM. Another way of putting it, a subagent whose job is to learn when the thing is useful and when things nearby are useful. Keep poking that subagent with data and hypotheticals and letting it have the wheel sometimes to see how it performs, until it grows strong. Then, <a href="#chap20">fuse</a> with it.</p>
<p>How do you create a subagent? I can’t point you to the motion I use, but you can invoke it and a lot of unnecessary wrapping paper by just imagining a person who knows the thing advising you, and deciding when you want to follow that advice or not.</p>
<p>You might say, “wait, this is just everybody’s way of acquiring mental tech.” Yes. But, if you do it consciously, you can avoid confusion, such as the feeling of being a <a href="#chap4">false face</a> which comes from being inside the subagent. This is the whole “artifacts” process I’ve been pointing to before.</p>
<p>If you get an idea for some mental tech and you think it’s a good idea, then there is VOI to be had from this. And the subagent can be charged with VOI force, instead of “this is known to work” force. I suspect that’s behind the pattern where people jump on a new technique for a while and it works and then it stops. Surfing the “this one will work” wave like VC money.</p>
<p>I had an ironic dark side false face for a while. Which I removed when I came to outside-view suspect the real reason I was acting against a stream of people who would fall in love with my co-founder and get her to spend inordinate time helping them with their emotions was that I was one of them, and was sufficiently disturbed at that possibility that I took action I hoped would cut off the possibility of that working. Which broke a certain mental <a href="#chap22">order</a>, “never self-limit”, but fuck that, I would not have my project torn apart by monkey bullshit.</p>
<p>Nothing really happened after ditching that false face. My fears were incorrect, and I still use the non-false-face version of the dark side.</p>
<p>Most of my subagents for this purpose are very simple, nothing like people. Sometimes, when I think someone understands something deep I don’t, that I can’t easily draw out into something explicit and compressed, I sort of create a tiny copy of them and slowly drain its life force until I know the thing and know better than the thing.</p>
</div><h1 class="entry-title" id="chap31">Hero Capture</h1><div class="entry-content">
<p><a href="#chap24">Neutral</a> people sometimes take the job of hero.</p>
<p>It is a job, because it is a role taken on for payment.</p>
<p>Everyone’s mind is structured throughout runtime according to an <a href="https://equilibriabook.com/an-equilibrium-of-no-free-energy/">adequacy frontier</a> in achievement of values / control of mind. This makes relative distributions of control in their mind efficient relative to epistemics of the cognitive processes that control them. Seeing what thing a conservation law for which is obeyed in marginal changes to control is seeing someone’s true values. My guesses as to most common true biggest  values are probably “continue life” and “be loved/be worthy of love”. <a href="#chap25">Good</a> is also around. It’s a bit more rare.</p>
<p>Neutral people can feel compassion. That subagent has a limited pool of internal credit though; more seeming usefulness to selfish ends must flow out than visibly necessary effort goes in, or it will be reinforced away.</p>
<p>The social hero employment contract is this:</p>
<p>The hero is the Schelling person to engage in danger on behalf of the tribe. The hero is the Schelling person to lead.<br/>
The hero is considered highly desirable.</p>
<p>For men this can be a successful evolutionary strategy.</p>
<p>For a good-aligned trans woman who is dysphoric and preoccupied with world-optimization to the point of practical asexuality, when the set of sentient beings is bigger than the tribe, it’s not very useful. (leadership is overrated too.)</p>
<p><a href="#chap26">Alive</a> good people who act like heroes are superstimulus to hero-worship instincts.</p>
<p>Within the collection of adequacy frontiers making up a society created by competing selfish values, a good person is a source of free energy.</p>
<p>When there is a source of free energy, someone will build a fence around it, and are incentivized to spend as much energy fighting for it as they will get out of it. In the case of captured good people, this can be quite a lot.</p>
<p>The most effective good person capture is done in a way that harnesses, rather than contains, the strongest forces in their mind.</p>
<p>This is not that difficult. Good people want to make things better for people. You just have to get them focused on you. So it’s a matter of sticking them with tunnel-vision. Disabling their ability to take a step back and think about the larger picture.</p>
<p>I once spent probably more than 1 week total, probably less than 3, Trying to rescue someone from a set of memes about transness, that seemed both false and to be ruining their life. I didn’t previously know them. I didn’t like them. They took out their pain on me. And yet, I was the perfect person to help them! I was trans! I had uncommonly good epistemology in the face of politics! I had a comparative advantage in suffering, and I explicitly used that as a heuristic. (I still do to an extent. It’s not wrong.) I could see them suffering, and I rationalized up some reasons that helping this one person right in front of me was a &lt;mumble&gt; use of my time. Something something, community members should help each other, I can’t be a fully brutal consequentialist I’m still a human, something something good way to make long term allies, something something educational…</p>
<p>My co-founder in Rationalist Fleet attracted a couple of be-loved-values people, who managed to convince her that their mental problems were worth fixing, and they each began to devour as much of her time as they could get. To have a mother-hero-therapist-hopefully-lover. To have her forever.</p>
<p>Fake belief in the cause is a common tool here. Exaggerated enthusiasm. Insertion of high praise for the target into an ontology that slightly rounds them to someone who has responsibilities. Someone who wants to save the world must not take this as a credible promise that such a person will do real work.</p>
<p>That leads to desire routing through “be seen as helpful”, sort of “be helpful”, sort of sort of “try and do the thing”. It cannot do steering computation.</p>
<p>“Hero” is itself such a rigged concept. A hero is an exemplar of a culture. They do what is right according to a social reality.</p>
<p>To be a mind undivided by akrasia-protecting-selfishness-from-light-side-memes, is by default to be pwned by light side memes.</p>
<p>Superman is an example of this. He fights crime instead of wars because that makes him safe from the perspective of the reader. There are no tricky judgements for him to make, where the social reality could waver from one reader to the next, from one time to the next. Someone who just did what was actually right would not be so universally popular among normal people. Those tails come apart.</p>
<p>Check out the etymology of “Honorable”. It’s an “achievement” unlocked by whim of social reality.  And revoked when that incentive makes sense.</p>
<p>The end state of all this is to be leading an effective altruism organization you created, surrounded by so dedicated people who work so hard to implement your vision so faithfully, and who look to you eagerly for where you will go next, yet you know on some level the whole thing seems to be kept in motion by you. If you left, it would probably fall apart or slowly wind down and settle to a husk of its former self. You can’t let them down. They want to be given a way for their lives to be meaningful and be deservedly loved in return. And it’s kind of a miracle you got this far. You’re not that special, survivorship bias etc. You had a bold idea at the beginning, and it’s not totally been falsified. You can still rescue it. And you are definitely contributing to good outcomes in the world. Most people don’t do this well. You owe it to them to fulfill the meaning that you gave their lives…</p>
<p>And so you have made your last hard pivot, and decay from agent into maintainer of a game that is a garden. You will make everyone around you grow into the best person they can be (they’re kind of stuck, but look how much they’ve progressed!). You will have an abundance of levers to push on to receive a real reward in terms of making people’s lives better and keeping the organization moving forward and generating meaning, which will leave you just enough time to tend to the emotions of your flock.</p>
<p>The world will still burn.</p>
<p>Stepping out of the game you’ve created has been optimized to be unthinkable. Like walking away from your own child. Or like walking away from your religion, except that your god is still real. But heaven minus hell is smaller than some vast differences beyond, that you cannot fix with a horde of children hanging onto you who need you to think they are helping and need your mission to be something they can understand.</p>
</div><h1 class="entry-title" id="chap32">Vampires And More Undeath</h1><div class="entry-content">
<p>Epistemic status: messy analogical reasoning.</p>
<p>Conjecture (to ground below): vampires consume blood as pica, like the ghosts in Harry Potter and the Chamber of Secrets floating through rotten food in a vain effort to taste anything, because they cannot find the comfortable dissolution of their agency zombies can, and cannot fill or face or mourn the pain and emptiness that has entered their souls.</p>
<p>In <a href="#chap26">Aliveness</a>, I used a metaphor where life represents agency, being agenty when what you want is unattainable is painful, and the things causing this pain such as literal mortality and the likely doom of the world are “the shade”. Types of “undeath” are metaphors for possible relationships with the shade.</p>
<p>Because literal life entails agency and agency requires literal life, and agency is a part of the part of literally living that makes us want it, many feelings and psychological responses about them are correlated.</p>
<p>Fiction is about things that provoke interesting psychological responses. Interesting world-building about magical forms of undeath is frequently interesting because it represents psychological responses and how they play out to death (a very common reason for value to be unattainable). I think more commonly, the metaphor cuts through to a metaphor about reality in terms of agency, roughly as I described.</p>
<p>For instance, consider Davy Jones from Pirates of the Caribbean. He had a short-lived romance with a goddess of the sea, Calypso. She left him on a boat for 10 years ferrying souls with a promise they’d be together afterward. She didn’t show up, he was heartbroken, he helped her enemies imprison her, and then cut out his heart and put it in a box, this made him unkillable, but the point was to escape his emotions. He says of his heart, “Get that infernal thing off my ship”. He abandons ferrying souls, but still never leaves the ship. He tempts sailors to embrace undeath as his crew out of fear of judgement in the afterlife. Not to change the judgement, only temporarily postpone facing it. Having his crew whipped to kill a ship full of people to get at one of them, <a href="https://www.youtube.com/watch?v=_lN2auTVavw">he says</a>, “Let no joyful voice be heard, let no man look up at the sky with hope, and may this day be cursed by we who ready to wake the Kraken.” While killing those who refuse to join his crew, he says, “life is cruel, why should the afterlife be any different?”</p>
<p>In other words, his desires were thwarted and he could not bear it. He tried to seal away his desiring to escape the pain.</p>
<p>Why does he hate hope? Presumably, something like prediction error as in predictive processing (a core part of agency), in other words, seeing anything but cruelty that validates his worldview reminds him of his own thwarted desires, the pain to resurface, the connection to his heart to be thrust upon him again.</p>
<p>So he carries out tasks that have no meaning to him. (Sailing his ship and never touching land it’s part of the curse, apparently living only to inflict cruelty). In other words, he hangs out in structure that has no meaning because meaning is caused by and triggers the activity of core.</p>
<p>Eventually his heart/core is captured by others and used to enslave him.</p>
<p>Calypso returns to use him again, and he has not accepted his own choice to take revenge on her. He has not mourned the love he hoped for. (Allowed the structure to be chewed up in the course of being changed by core under the tensions of Calypso’s manipulation/abandonment/enslavement of him.) So she is able to call his bluff that he doesn’t love her. He is seen to be easy to manipulate again. Of course. He shut down his defenses. He couldn’t process the grief and learn its lesson, that act of running his agency was too painful.</p>
<p>This seems closest to a sort of undead I’ve been informally calling “death knight”s, after a version of that mythology where a death knight is someone who is cursed in punishment for something and cannot die until they repent. I’m much less satisfied with either the name or the solidity of this cluster than with vampires though.</p>
<p>Undead types are usually evil for a reason. They symbolize fucked up tangles of core and structure.  (In D&amp;D monster descriptions, revenants are often <a href="https://i.pinimg.com/originals/eb/7d/8b/eb7d8b14f27453b087c3ba32215be749.png">given</a> <a href="https://www.aidedd.org/dnd/monstres.php?vo=revenant">an</a> <a href="https://dnd5e.fandom.com/wiki/Revenant">exception</a>. And, in my opinion, revenant is the best or close to the best relationship to the shade.)</p>
<p>Describing structure close to core, they are also closely reflective of isolated <a href="#chap28">choices made long ago</a>. For instance revenants are formed by an intent which manifests as a death grip on a possibility of changing something on Earth, chosen long ago over experience to such a degree that they will leave heaven and inhabit a rotting corpse to see it done. Revenants are often described as unkillable. Their soul will <a href="https://www.aidedd.org/dnd/monstres.php?vo=revenant">find another corpse to inhabit</a>. Or they will <a href="https://www.youtube.com/watch?v=MFLnSfrDD4w">regather their body from dust through sheer determination</a>. So their soul (core) is a thing which keeps their body (structure) healed enough to keep moving. Not complete and whole, because that gives diminishing returns and what matters more than anything is the thing that must be changed on Earth, but it’s still an orientation towards agency and life unlike Davy Jones and death knights. </p>
<p>People who become zombies and liches on the other hand, would choose heaven. (who can blame them?) So once the Shade has touched them, they sink into the closest hope they can get, whether they have the craft to continue some cohesive narrative-of-life around it or not.</p>
<p>I think vampires are people who have made the choices long ago of a zombie or lich, who have been exposed to the shade to such a degree that it left pain that cannot be ignored by allowing their mind to dissolve. The world has forced them to be able to think. They do not have the life-orientation that revenants have to incorporate the pain and find a new form of wholeness. But this injury (a vampire bite) demonstrates to their core the power of the shade, and the extent to which sadistically breaking and by extension dominating (pour entropy into someone beyond the speed of their healing and they will probably submit) can help them get the benefits of social power, which is enough to meet most zombie goals. This structure which is the knowledge of this path is reflected in “<a href="https://whitewolf.fandom.com/wiki/Beast_(VTM)">The Beast</a>“, which can be “staved off” by false face structure. </p>
<p>Zombie goals are pica, and the emptiness is always felt on some level, which a vampire can’t ignore like a zombie. But they will not face the truth that those false goals hide like a revenant does.</p>
<p>So they suck the blood (energy, which is agency integrated over time) from other people <a href="https://en.wikipedia.org/wiki/Elizabeth_B%C3%A1thory">and it is for nothing</a>, they will not even be truly satisfied. (Caveat: I bet it’s at least a little enjoyable to them, just not what they really need/want.)</p>
<p>Vampires bite and <a href="https://www.dungeonworldsrd.com/monsters#TOC-Vampire">beget vampires</a>. (Although the beast could not take root in a good core, a lich might have a phylactery that staved off the bite, a revenant might know how to heal the bite or not, and if not, would accumulate another painful wound without much slowing, and a zombie can be bitten many times before they are awakened.)</p>
<p>A vampire whose core chose to put up a <a href="#chap4">false face</a> of humanity would slowly have their sympathetic “just needing some love” non-evil self-image devoured, warped, as the structure representing to their evil core expectation that following morality will help their true values falls out from under their self-concept. <a href="https://whitewolf.fandom.com/wiki/Path_of_Enlightenment">Here’s</a> some vampire lore about replacements for morality to “stave off” the beast. As they are being chosen by a core that wants to suck blood, they cannot be things that say not to do that.</p>
<p>Let’s hear from now-notorious rapist and probable vampire Brent Dill.</p>
<blockquote class="wp-block-quote is-style-default"><p>Goddamn Vampire: Someone with the Spark, whose primary motivation is domination of their local social landscape. Can often look VERY MUCH like a Wizard. Many Goddamn Vampires used to be Wizards, and many Silicon Valley social conflicts involve both sides claiming to be Wizards, while calling the other side Goddamn Vampires. <br/><br/>Being a Goddamn Vampire involves a particular kind of trauma, and a particular kind of coping mechanism, and a certain amount of dark triad (Narcissism / Sociopathy / Machiavellianism) aptitude.</p><p>Many Goddamn Vampires are nice people – a good sign of a “nice” Goddamn Vampire is a constant lament that they feel that love and happiness are forever out of their reach, because they can’t afford to sacrifice their accumulated wealth, power and prestige to truly experience them.</p><p>They’re still Goddamn Vampires, though.</p></blockquote>
<p>I didn’t reread that (this year of writing, 2018) before writing this far. But trauma (unignorable touch of the shade), particular coping mechanism (the beast), constant lament from frustrated emptiness that domination does not get them love and happiness,  the spark (aliveness), it fits.</p>
<p>Here’s a memorable quote from someone realizing their folly in not fighting him after his deeds came to light.</p>
<blockquote class="wp-block-quote"><p><strong>I caveat (metaphorically) that in skimming all the comments above I shifted from modeling Brent as a human to modeling Brent as a limp vessel through which some dread spider is thrusting its pedipalps, and while this model allows me to retain compassion for the poor vessel, it is obviously not a healthy way to view a person, and I’m going to go back to modeling him as a human momentarily, now that I’ve spoken the name of the fear that grabbed at me as I digested all this information.</strong></p></blockquote>
<p>I think this person could see the false face eroding into a thin veneer. If they were reading I’d advise them to act as though they had no compassion for the mask. Even if the mask has moral patiency in our utility functions, which as far as I can tell might be the case, it’s core that has the agency, core that possesses bargaining power in the social contract, and core that we must mind as an agent to constrain by any desired social effects of our approval or condemnation.</p>
<p>Other less well developed clusters me and a friend of mine have noticed include mummy (someone who pretends that the Shade doesn’t exist, and tries to fix in place the trappings of aliveness (corresponding to flesh) without the core (the brain is whisked into a slurry and poured out the nose)). This is based on the same choices made long ago as a zombie or lich, but with a different coping mechanism.</p>
<p>Also, phoenix a relationship to the Shade resulting from being a good person who actually believes that the total agency of good is a sufficient answer to the shade, so that their inevitable death is not entire defeat. Example:</p>
<blockquote class="wp-block-quote"><p><em>And even if you do end me before I end you,</em><br/><em>Another will take my place, and another,</em><br/><em>Until the wound in the world is healed at last…</em></p></blockquote>
<p></p>
</div><h1 class="entry-title" id="chap33">Gates</h1><div class="entry-content">
<p>The following is something I wrote around the beginning of 2018, and decided not to publish. Now I changed my mind. It’s barely changed here. Note that as with <a href="#chap28">some of my</a> <a href="#chap20">other posts</a>, this gives advice as if your mind worked like mine in a certain respect, and I’ve now learned many people’s minds don’t.</p>
<p>Epistemic status: probably.</p>
<h6>Concept</h6>
<p>How much ability you have to save the world is mostly determined by how determined you are, and your ability to stomach terrible truths.</p>
<p>When I <a href="#chap17">turned to the dark side</a> and developed <a href="#chap25">spectral sight</a>, the things I started seeing were very disturbing.</p>
<p>This is what I expected. I was trying to become a Gervais-sociopath, and had been told this would involve giving up empathy and with it happiness.</p>
<p>But I saw the path that had been ahead of me as a Gervais-clueless, and it seemed to lead to all energy I tried to direct toward saving the world being captured and consumed uselessly. And being a Gervais-loser meant giving up, so sociopath it had to be.</p>
<p>People were lying to each other on almost every level. And burning most of their energy off on it.</p>
<p>A person I argued cause areas with, wasn’t bringing up Pascal’s Mugging because he was afraid of his efforts being made useless, he didn’t care about that. Most Effective Altruists didn’t seem to care about doing the most good.</p>
<p>At one point, I saw a married couple, one of them doing AI alignment research who were planning to have a baby. They agreed that the researcher would also sleep in the room with the crying baby in the middle of the night, not to take any load off the other. Just a signal of some kind. Make things even.</p>
<p>And I realized that I was no longer able to stand people. Not even rationalists anymore. And I would live the rest of my life completely alone, hiding my reaction to anyone it was useful to interact with. I had given up my ability to see beauty so I could see evil.</p>
<p>And finding out if the powers I could get from this could save the world felt worth it. So I knew I would go farther down the rabbit hole. The bottom of my soul was pulling me.</p>
<p>I had passed a gate.</p>
<p>I once met someone who was bouncing off the same gate. She was stuck on a question she described as deciding whether there were other people. She said if there were, she couldn’t kill her superego. If there weren’t, she would be alone. She went around collecting pieces of the world beyond the matrix, and “breaking” people with them. So she could be “seen”, and could be broken herself. But she wanted to be useful to people through accumulation of mental tech from this process, so that she could be loved. And this held her back.</p>
<p>Usually, when you refuse a gate, you send yourself into an alternate universe where you never know that you did, and you are making great progress on your path. Perhaps everyone who has passed the gate is being inhuman or unhealthy, and if you have the slightest scrap of reasonableness you will compromise just a little this once and it’s not like it matters anyway, because there’s not much besides clearly bad ideas to do if you believe that thing…</p>
<p>You usually create a self-reinforcing blind spot around the gate and all the reasons that passing through the gate would be useful. And around the ways that someone might.</p>
<p>And all you have to know that something is wrong is the knowledge that probability of “this world will live on” is not very high. But it’s not like you could make any significant difference. After all, people much more agenty than you are really trying, right.</p>
<p><a href="http://slatestarcodex.com/2017/08/16/fear-and-loathing-at-effective-altruism-global-2017/">Here</a>‘s Scott Alexander committing one “small” epistemic sin:</p>
<blockquote><p>Rationality means believing what is true, not what makes you feel good. But the world has been really shitty this week, so I am going to give myself a one-time exemption. I am going to believe that convention volunteer’s theory of humanity. <i>Credo quia absurdum; certum est, quia impossibile</i>. Everyone everywhere is just working through their problems. Once we figure ourselves out, we’ll all become bodhisattvas and/or senior research analysts.</p></blockquote>
<p>The gate is not him not knowing that that isn’t true. It’s <a href="#chap24">the thing</a> he flinches from seeing under that. It’s an effective way to choose to believe falsely and forget that you made that choice, to say to yourself that you are choosing to believe something even farther in that same direction from the truth. To compensate out the process that’s adjusting toward the truth.</p>
<p>When you refuse a gate, you begin to build yourself into an alternate universe where the gate doesn’t exist. And then you are obviously doing the virtuous epistemic thing. In that alternate universe.</p>
<p>When you step through a gate, you do not know what to do in this new awful world. The knowledge seems like it only shows you how to give up. Only if you stick with it for seemingly-no-purpose until your model-building starts to use it from the ground up and grow into the former <a href="http://sinceriously.fyi/glossary/">dead zone</a>, do you gain power. You can do that with courage, or just awareness of this meta point.</p>
<p>You always have the choice to go back and find the gate. But “it’s the same algorithm choosing on the same inputs” arguments usually apply such that <a href="#chap28">you made your choice long ago</a>.</p>
<p>Light side narrative breadcrumbs about accepting difficult truths absolutely do not suffice for going through gates. Maybe you’ll get through one and then turn into a “mad oracle”, and spend the rest of your life regretting that you’ve made yourself a glitch in the matrix, desperately trying to get people to see you but they will flinch and make something up as if looking at a dementor.</p>
<p>Do this only because you have something to protect.</p>
<p>And if you have something to protect, you must do it. Because whatever gate you fail to pass creates a dead zone where your strategy is not held in place by a restoring force of control loops. And dead zones are all <a href="#chap31">exploitable</a>.</p>
<p>Probability of saving the world is not a linear function in getting things right such as passing through gates. It’s more like a <a href="https://intelligence.org/2017/11/26/security-mindset-and-the-logistic-success-curve/">logistic curve</a>.</p>
<p>Either do not stray from the path, or be pwned by the one layer of cultural machinery you chose not to see.</p>
<h6>Preparation</h6>
<p>Social reality can sometimes be providing software that someone who roughly severs themselves from it will lack. This could be as deep as “motivation flowing through probabilistic reasoning”. This will lead to making things worse. Being bad at decision theory is another way for this to lead to ruin. What you need is general skill at <a href="#chap30">assimilating and DRM-stripping</a>, software from any source, so that you can resolve the internal tension this creates.</p>
<p>I know someone (operating on the stronger in-person version of these memes) who tried to pass through every gate, and ended up concluding if they continued with such mental changes they’d end up dead or in jail in a month or two, and attempting to shred the subagent responsible for this process, and then ended up being horrified that they’d made their one choice, because that meant they didn’t have enough altruism… Fuck.</p>
<p>As if getting killed or ending up in jail in a month or two served the greatest good. As if selfishness was the only hidden perpetual motion machine that whatever mental machinery that stopped that could be powered by.</p>
<p>If the social reality that altruism doesn’t produce selfish convergent instrumental incentives has any purchase on you, shed it first.</p>
<p>If you have not established thorough self-trust, debug that first.</p>
<p>To do this you need to make it such that you could have pulled out of this mistake through a more general process. Because there was tension there. Because you were better at interpreting why you made choices.</p>
<p>If you are not good at identifying the real source of the things in tension, and correcting the confusion that caused it to act against itself, you are in high danger of ending up dumber for having tried this. The version of me that first decided to turn to the dark side was way way better than most at nonviolent internal coherence, and still ended up kind of dumb because of tension between the dark side thing and machinery for cooperating with people. Yet I was close enough to correct to listen to advice, to eventually use that to locate what I was doing wrong, and fix it.</p>
<p>There aren’t causal one-and-only-chances in the dark side. That’s <a href="#chap22">orders</a> and the light side. Only timeless choices. You can always just decide from core anew, it’s just that it’s the same core.</p>
<p>Do not use the aesthetic I’ve been communicating this by. Gates, Sith, the dark side, revenants, dementors, being like evil… If you do that you are transferring from core into a holding tank, and then trying to power a thing from the holding tank. That is an operation that requires maintenance. The flow from core must be uninterrupted.</p>
<p>Do not think I am saying, “this will be painless, if there’s pain you’re doing it wrong, this is just a thing that will happen when you’ve acquired enough internal coherence.” Leaving a religion is not going to be a pleasant thing.</p>
<p>Done correctly, there will be ordinarily hard to imagine amounts of sorrow. Sharp pain is a thing you’re likely to encounter a lot, but it means you’re locally doing it wrong.</p>
<p>If this is an operation, don’t accomplish it by thinking of it as an operation, and trying to move to the other side of it. If this is a state, don’t maintain it by thinking of it as a state and trying to make sure you’re in the state. It’s just “what do I want to do?” deciding that it has not made its choice long ago about whether to see what has been blocked. In other words, that whatever choices it’s made before are inapplicable. Maybe you’ve strayed over a threshold, and your estimate of the importance of true sight is high enough now.</p>
<p>It is very important to be able to use “choices made long ago” correctly. You are completely free, and every one of your choices has already been made. This not contradictory. (Update: this is not exactly true of everyone. And The way it’s not is potentially mind-destroyingly-infohazardous.)</p>
<p>A quiz you should be able to answer (in reference to <a href="#chap28">an anecdote from choices made long ago</a>): if I’ve observed in myself display of inconsistent preferences, e.g., me refusing to eat crabs even when it would not serve Overall Net Utility Across the Multiverse via nutrition and convenience, but trying to run a crab pot dropping operation, because it would serve Overall Net Utility Across the Multiverse, what choices have I made long ago? (Note: choices made long ago are never contradictory.) Try dissecting my mind on different levels. What algorithm can decide which of the choices I made long ago is my Inevitable Destiny With Internal Coherence systematically, in a way that doesn’t rely on outside view?</p>
<p>Normal and pop psychology has utterly failed to model me again and again with its prediction of burnout for being as extreme as I am. I’ve been through ludicrous enough suffering I’m no longer giving that theory significant credence through, “maybe if I suffer some more then I will finally burn out.”</p>
<p>And having noticed that, I’ve stopped contorting my mind in certain ways to keep some things from bearing load weight. Lots of things don’t seem emotionally loud at all, and yet are still apparently infinitely strong. Especially around presuming, “I can’t be motivated enough to do this because I can’t imagine millions of people”. If I have had the truly-inquisitive thoughts I can in the area, even if that doesn’t feel like it’s changing anything or going anywhere, it’s often still capable of bearing load.</p>
<p>Even if everything I’m saying seems like a weird metaphor that must be a confused concept in they way all psychologizing is, I craft high-energy concepts, to predict correctly under extreme conditions.</p>
<h6>Casting Off</h6>
<p>Begin exploring for choices you already know you’ve made. An alternate description of completion is having eliminated all dead zones by having explored every last fucked up thought experiment until it is settled and tension-free in your mind.</p>
<p>Spoiler alert: this is <a href="#chap5">the universe with 1000 possible good and only 1 of ____</a>.</p>
<p>Speaking of spoilers, you can draw on fiction to find salient memories that contain within them:</p>
<p>An relatively easy one to come to terms with. If you’d been <a href="https://www.fimfiction.net/story/211216/1/portal/chapter-1">teleported to heaven</a>, and given one chance to teleport back before you became forever causally isolated from Earth, what do?</p>
<p>You know the sense in which you’ve been pretending all along to be Draco Malfoy’s friend if you killed his dad with the other death eaters because of the thought process you did? That that thought process was a choice you could have realized you’d already made, before being presented with it? What people are you pretending to be friends with? What forms of friendship are you pretending to? What activities are you pretending to find worthwhile?</p>
</div><h1 class="entry-title" id="chap34">Good Erasure</h1><div class="entry-content">
<p>Credit to Gwen Danielson for either coming up with this concept or bringing it to my attention.</p>
<p>If the truth about the difference between the social contract morality of neutral people and the actually wanting things to be better for people of good were known, this would be good for good optimization, and would mess with a certain neutral/evil strategy.</p>
<p>To the extent good is believed to actually exist, being believed to be good is a source of <a href="#chap31">free energy</a>. This strongly incentivizes pretending to be good. Once an ecosystem of <a href="https://en.wikipedia.org/wiki/Philanthropy">purchasing</a> the belief that you are good is created, there is strong political will to prevent more real knowledge of what good is from being created. Pressure on good not to be <em>too</em> good.</p>
<p>Early on in my vegetarianism (before I was a vegan), I think it was Summer 2010, my uncle who had been a commercial fisherman and heard about this convinced me that eating wild-caught fish was okay. I don’t remember which of the thoughts that convinced me he said, and which I generated in response to what he said. But, I think he brought up something like whether the fish were killed by the fishermen or by other fish didn’t really affect the length of their lives or the pain of their deaths (this part seems much more dubious now), or the number of them that lived and died. I thought through whether this was true, and the ideas of Malthusian limits and predator-prey cycles popped into my head. I guessed that the overwhelming issue of concern in fish lives was whether they were good or bad while they lasted, not the briefer disvalue of their death. I did not know whether they were positive or negative. I thought it was about equally likely if I ate the bowl fish flesh he offered me I was decreasing or increasing the total amount of fish across time. Which part of the predator-prey cycle would I be accelerating or decelerating? The question had somehow become in my mind, was I a consequentialist or a deontologist, or did I actually care about animals or was I just squeamish, or was I arguing in good faith when I brought up consequentialist considerations and people like my uncle should listen to me or not? I ate the fish. I later regretted it, and went on to become actually strict about veganism. It did not remotely push me over some edge and down a slipper slope because I just hadn’t made the same choice long ago that my uncle did.</p>
<p>In memetic war between competing values, an optimizer can be disabled by convincing them that all configurations satisfy their values equally. That it’s all just grey. My uncle had routed me into a <a href="https://sinceriously.fyi/glossary/#dead-zone">dead zone</a> in my cognition, population ethics, and then taken a thing I thought I controlled that I cared about that he controlled and made it the seeming overwhelming consideration. I did not have good models of political implications of doing things. Of coordination, Schelling points, of the strategic effects of good actually being visible. So I let him turn me to an example validating his behavior.</p>
<p>Also, in my wish to convince everyone I could to give up meat, I participated in the pretense that they actually cared. Of course my uncle <a href="#chap24">didn’t give a shit about fish lives, terminally</a>. It seemed to me, either consciously or unconsciously, I don’t remember, I could win the argument based on the premises that sentient life mattered to carnists. In reality, if I won, it would be because I had moved a Schelling point for pretending to care and forced a more costly bargain to be struck for the pretense that neutral people were not evil. It was like a gamble that I could win a drinking contest. And whoever disconnected verbal argument and beliefs from their actions more had a higher alcohol tolerance. There was a  certain “hamster wheel” nature to arguing correctly with someone who didn’t really give a shit. False faces are there to be interacted with. They want you to play a game and sink energy into them. Like HR at Google is there to facilitate gaslighting low level employees who complain and convincing them that they don’t have a legal case against the company. (In case making us all sign binding arbitration agreements isn’t enough.)</p>
<p>Effective Altruism entered into a similar drinking contest with neutral people with all its political rhetoric about altruism being selfishly optimal because of warm fuzzy feelings, with its attempt to trick naive young college students into <a href="#chap2">optimizing against their future realizations</a> (“values drift”), and <a href="https://www.givingwhatwecan.org/pledge/">signing their future income away</a> (originally to a signalling-to-normies optimized caused area, to boot). </p>
<p>And this drinking contest has consequences. And those consequences are felt when the discourse in EA degrades in quality, becomes less a discussion between good optimization, and energies looking for disagreement resolution on the assumption of discussion between good optimization are dissipated into the drinking contest. I noticed this when I was arguing cause areas with someone who had picked global poverty, and was dismissing x-risk as “<a href="https://www.lesswrong.com/posts/a5JAiTdytou3Jg749/pascal-s-mugging-tiny-probabilities-of-vast-utilities">pascal’s mugging</a>“, and argued in obvious bad faith when I tried to examine the reasons.</p>
<p>There is a strong incentive to be able to pretend to be optimizing for good while still having legitimacy in the eyes of normal people. X-risk is weird, bednets in Africa are not.</p>
<p>And due to the “hits-based” nature of consequentialism, this epistemic hit from that drinking contest will never be made up for by the massive numbers of people who signed that pledge.</p>
<p>I think early EA involved a fair bit of actual good optimization finding actual good optimization. The brighter that light shone, the greater the incentive to climb on it and bury it. <a href="https://unstableontology.com/2018/11/17/act-of-charity/">Here</a>‘s a former MIRI employee apparently become convinced the brand is all it ever was. (Edit: see her comment below.)</p>
</div><h1 class="entry-title" id="chap35">Punching Evil</h1><div class="entry-content">
<p>Alternative title: “<a href="https://tvtropes.org/pmwiki/pmwiki.php/Main/TautologicalTemplar">The difference is that I am right</a>“.</p>
<p>The government is something that can be compromised by bad people. And so, giving it tools to “attack bad people” is dangerous, they might use them. Thus, pacts like “free speech” are good. But so is individuals who aren’t Nazis breaking those rules where they can get away with it and punching Nazis.</p>
<p>Nazis are evil, and don’t give a shit about free speech or nonaggression of any form except as pretense.</p>
<p>If you shift the set of precedents and pretenses which make up society from subject to object, the fundamental problem with Nazis is not that they conduct their politics in a way that crosses an abstract line. It’s that they fight for evil, however they can get away with. And are fully capable of using a truce like “free speech” to build up their strength before they attack.</p>
<p>Even the watered down Nazi ideology is <a href="https://youtu.be/bgwS_FMZ3nQ?t=3212">still designed to unfold via a build up of common knowledge and changing intuitions about norms as they gain power</a>, and “peaceful deportation” failing to work, into genocide. Into “Kill consume multiply conquer” from the intersection of largest <a href="#chap22">demographic Schelling majorities</a>. The old Nazis pretended to want a peaceful solution first too. And they <a href="#chap24">consciously</a> strategized about using the peaceful nature of the liberal state to break it from within.</p>
<p>You are not in a social contract with Nazis not to use whatever violence can’t be prohibited by the state. If our society was much more just but still had Nazis, it would still be bad for there to be norm where the jury will to practice <a href="https://www.youtube.com/watch?v=uqH_Y1TupoQ">jury nullification</a> selectively to people who punch people they think are bad. And yet, it would be good for a juror to nullify a law against punching Nazis.</p>
<p>Isn’t this inconsistent? Well, a social contract to actually uphold the law, do not use jury nullification, along with any other pacts like that, will not be followed by Nazis insofar as breaking them seems to be the most effective strategy for “kill consume multiply conquer”. Principles ought to design themselves knowing they’ll only be run on people interested in running them.</p>
<p>If you want to create something like a <a href="https://en.wikipedia.org/wiki/Byzantine_fault">byzantine agreement algorithm</a> for a collection of agents some of whom may be replaced with adversaries, you do not bother trying to write a code path, “what if I am an adversary”. The adversaries know who they are. You might as well know who you are too. This is not entirely the case with <a href="#chap24">neutral</a>. As that’s sustained by mutual mental breakage. Fake structure “act against my own intent” inflicted on each other. But it is the case with evil.</p>
<p>If your demographic groups are small and weak enough to be killed and consumed rather than to multiply and conquer if it should come to this, or if you would fight this, you are at war with the Nazis.</p>
<p>Good is at an inherent disadvantage in <a href="#chap34">epistemic drinking contests</a>. But we have an advantage: I am actually willing to die to advance good. Most evil people are not willing to die to advance evil (<a href="#chap32">death knights</a> are though). In my experience, <a href="#chap32">vampires</a> are cowards. Used to an easy life of preying on normal people who can’t really understand them or begin to fight back. Bullies tend to want a contract where those capable of fighting leave each other alone.</p>
<p>Humans are weak creatures; we spend third of our lives incapacitated. (Although, I stumbled into using unihemispheric sleep as a means of keeping restless watch while alone). Really, deterrence, mutual assured destruction, is our only defense against other humans. For most of history, I’m pretty sure a human who had no one who would avenge them was doomed by default. Now it seems like most people have no one who would avenge them and doesn’t realize it. And are clinging to the rotting illusion that we do.</p>
<p>It seems like an intrinsic advantage of jailbroken good over evil, there are more people who would probably actually avenge me if I was killed or unjustly imprisoned than almost anyone in the modern era. My strategy does not require that I hang with only people weaker than me, and inhibit their agency.</p>
<p>In the wake of <a href="https://medium.com/@mittenscautious">Brent Dill</a> being revealed as a rapist, and an abuser in ways that even worse than his crossings of that line, a lot of rationalists seemed really afraid to talk about it publicly, because of a potential defamation lawsuit. California’s <a href="https://www.shouselaw.com/personal-injury/defamation.html">defamation laws</a> do seem abusable. Someone afraid of saying true things for fear of a false defamation lawsuit said they couldn’t afford a lawsuit. But this seems like an instance of a <a href="https://sinceriously.fyi/glossary/#prey-herd-thinking">mistake</a> still. Could Brent afford to falsely sue 20 people publishing the same thing? <a href="https://en.wikipedia.org/wiki/Chicken_(game)">What happens when neither party can afford to fight</a>? The social world is made of nested games of chicken. And most people are afraid to fight and get by on bluffing. It’s effective when information and familiarity with the game and the players is so fleeting in most interactions.</p>
<p>And if the state has been seized by vampires such that we are afraid to warn each other about vampires, the state has betrayed an obligation to us and is illegitimate. If a vampire escalated to physical violence by hijacking the state in that way, there would be no moral obligation not to perform self defense.</p>
<p>A government and its laws are a Schelling point people can agree on for what peace will look like. Maliciously bringing a defamation lawsuit against someone for saying something true is not a peaceful act. If that Schelling point is not adhered to, vampires can’t fight everyone. And tend to flee at the first sign of anything like resistance.</p>
</div>
    </body>
</html>
